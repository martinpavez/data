{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Top Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Desktop\\data\\hw_extra\n"
     ]
    }
   ],
   "source": [
    "# Add the folder to the Python path\n",
    "\n",
    "os.chdir(\"../\")\n",
    "# change working directory to project's root path\n",
    "print(os.getcwd())\n",
    "\n",
    "folder_path = os.path.abspath(\"functions/\") #INPUT_PATH)#'path_to_your_folder')  # Replace with the actual folder path\n",
    "sys.path.insert(0, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PredictorsDrivers import (\n",
    "    PCAPredictors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are just going to use ONI, PDO and SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oni = pd.read_csv(\"data/indices/oni.txt\",sep=\"   |  | \", header=None, engine=\"python\", names=[\"Season\", \"Year\", \"sst\", \"ONI\"])\n",
    "df_pdo = pd.read_csv(\"data/indices/pdo.dat\",sep=\"  | \", header=None, engine=\"python\", names=[\"Year\"]+[str(i) for i in range(1,13)])\n",
    "df_sam = pd.read_csv(\"data/indices/sam.txt\",sep=\"  | \", header=None, engine=\"python\", names=[\"Year\"]+[str(i) for i in range(1,13)])\n",
    "\n",
    "season_to_month = {\n",
    "    \"DJF\": \"02\", \"JFM\": \"03\", \"FMA\": \"04\", \"MAM\": \"05\",\n",
    "    \"AMJ\": \"06\", \"MJJ\": \"07\", \"JJA\": \"08\", \"JAS\": \"09\",\n",
    "    \"ASO\": \"10\", \"SON\": \"11\", \"OND\": \"12\", \"NDJ\": \"01\"\n",
    "}\n",
    "df_oni[\"Year\"] = df_oni.apply(lambda row: row[\"Year\"] + 1 if row[\"Season\"] == \"NDJ\" else row[\"Year\"], axis=1)\n",
    "df_oni[\"month\"] = df_oni[\"Season\"].map(season_to_month)\n",
    "df_oni[\"Date\"] = pd.to_datetime(df_oni[\"Year\"].astype(str) + \"-\" + df_oni[\"month\"])\n",
    "\n",
    "# Select required columns\n",
    "df_oni = df_oni[[\"Date\", \"ONI\"]]\n",
    "df_oni.set_index(\"Date\", inplace=True)\n",
    "\n",
    "df_pdo = df_pdo.melt(id_vars=['Year'], var_name='Month', value_name='PDO')\n",
    "df_pdo[\"Month\"] = pd.to_numeric(df_pdo[\"Month\"])\n",
    "df_pdo = df_pdo.sort_values(['Year','Month'])\n",
    "\n",
    "df_pdo['Date'] = pd.to_datetime(df_pdo[['Year', 'Month']].assign(DAY=1))\n",
    "df_pdo.set_index(\"Date\", inplace=True)\n",
    "df_pdo.drop(columns=[\"Year\", \"Month\"],inplace=True)\n",
    "\n",
    "df_sam = df_sam.melt(id_vars=['Year'], var_name='Month', value_name='SAM')\n",
    "df_sam[\"Month\"] = pd.to_numeric(df_sam[\"Month\"])\n",
    "df_sam = df_sam.sort_values(['Year','Month'])\n",
    "\n",
    "df_sam['Date'] = pd.to_datetime(df_sam[['Year', 'Month']].assign(DAY=1))\n",
    "df_sam.set_index(\"Date\", inplace=True)\n",
    "\n",
    "df_sam.drop(columns=[\"Year\", \"Month\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdo = df_pdo[(df_pdo.index.year >= 1972) & (df_pdo.index.year <=2022)]\n",
    "df_oni = df_oni[(df_oni.index.year >= 1972) & (df_oni.index.year <=2022)]\n",
    "df_sam = df_sam[(df_sam.index.year >= 1972) & (df_sam.index.year <=2022)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chile SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"chile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the experiments to after incorporate the indices in every experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_6means = xr.load_dataset(\"data/local_data/7means_world.nc\")\n",
    "name_pcas = \"pcas_1972.pkl\"\n",
    "with open(name_pcas, \"rb\") as inp:\n",
    "    pcas = pickle.load(inp)\n",
    "predictors = PCAPredictors(ds_6means, 3, saved_pcas=pcas,total_variables=[\"SP\", \"TTR\", \"U10\", \"V10\", \"Z\", \"SST\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of different PCAS 4536\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total of different PCAS {len(predictors.df_predictors.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 experiments\n"
     ]
    }
   ],
   "source": [
    "predictors.load_experiments(f\"data/new_features/{region}\", f\"data/new_features/{region}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "for exp in list(predictors.experiments.keys()):\n",
    "    experiment, num = predictors.incorporate_predictor(exp, [df_pdo[\"PDO\"], df_oni[\"ONI\"], df_sam[\"SAM\"]], [\"PDO\", \"ONI\", \"SAM\"])\n",
    "    predictors.experiment_to_parquet(num, f\"data/new_features/{region}\", f\"data/new_features/{region}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chile T2M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the experiments to after incorporate the indices in every experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del predictors\n",
    "del pcas\n",
    "name_pcas = \"pcas_t2m.pkl\"\n",
    "with open(name_pcas, \"rb\") as inp:\n",
    "    pcas = pickle.load(inp)\n",
    "predictors = PCAPredictors(ds_6means, 3, saved_pcas=pcas,total_variables=[\"SP\", \"TTR\", \"U10\", \"V10\", \"Z\", \"T2M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of different PCAS 4536\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total of different PCAS {len(predictors.df_predictors.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 experiments\n"
     ]
    }
   ],
   "source": [
    "predictors.load_experiments(f\"data/new_features_t2m/{region}\", f\"data/new_features_t2m/{region}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "for exp in list(predictors.experiments.keys()):\n",
    "    experiment, num = predictors.incorporate_predictor(exp, [df_pdo[\"PDO\"], df_oni[\"ONI\"], df_sam[\"SAM\"]], [\"PDO\", \"ONI\", \"SAM\"])\n",
    "    predictors.experiment_to_parquet(num, f\"data/new_features_t2m/{region}\", f\"data/new_features_t2m/{region}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"california\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the experiments to after incorporate the indices in every experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del predictors\n",
    "del pcas\n",
    "name_pcas = \"pcas_1972.pkl\"\n",
    "with open(name_pcas, \"rb\") as inp:\n",
    "    pcas = pickle.load(inp)\n",
    "predictors = PCAPredictors(ds_6means, 3, saved_pcas=pcas,total_variables=[\"SP\", \"TTR\", \"U10\", \"V10\", \"Z\", \"SST\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of different PCAS 4536\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total of different PCAS {len(predictors.df_predictors.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 experiments\n"
     ]
    }
   ],
   "source": [
    "predictors.load_experiments(f\"data/new_features/{region}\", f\"data/new_features/{region}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "for exp in list(predictors.experiments.keys()):\n",
    "    experiment, num = predictors.incorporate_predictor(exp, [df_pdo[\"PDO\"], df_oni[\"ONI\"], df_sam[\"SAM\"]], [\"PDO\", \"ONI\", \"SAM\"])\n",
    "    predictors.experiment_to_parquet(num, f\"data/new_features/{region}\", f\"data/new_features/{region}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California T2M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the experiments to after incorporate the indices in every experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del predictors\n",
    "del pcas\n",
    "name_pcas = \"pcas_t2m.pkl\"\n",
    "with open(name_pcas, \"rb\") as inp:\n",
    "    pcas = pickle.load(inp)\n",
    "predictors = PCAPredictors(ds_6means, 3, saved_pcas=pcas,total_variables=[\"SP\", \"TTR\", \"U10\", \"V10\", \"Z\", \"T2M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of different PCAS 4536\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total of different PCAS {len(predictors.df_predictors.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 experiments\n"
     ]
    }
   ],
   "source": [
    "predictors.load_experiments(f\"data/new_features_t2m/{region}\", f\"data/new_features_t2m/{region}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for exp in list(predictors.experiments.keys()):\n",
    "    experiment, num = predictors.incorporate_predictor(exp, [df_pdo[\"PDO\"], df_oni[\"ONI\"], df_sam[\"SAM\"]], [\"PDO\", \"ONI\", \"SAM\"])\n",
    "    predictors.experiment_to_parquet(num, f\"data/new_features_t2m/{region}\", f\"data/new_features_t2m/{region}/metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
