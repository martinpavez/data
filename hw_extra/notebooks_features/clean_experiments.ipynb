{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from metpy.calc import advection\n",
    "from metpy.units import units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Desktop\\data\\hw_extra\n"
     ]
    }
   ],
   "source": [
    "# Add the folder to the Python path\n",
    "\n",
    "os.chdir(\"../\")\n",
    "# change working directory to project's root path\n",
    "print(os.getcwd())\n",
    "\n",
    "folder_path = os.path.abspath(\"functions/\") #INPUT_PATH)#'path_to_your_folder')  # Replace with the actual folder path\n",
    "sys.path.insert(0, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndexDrivers import (\n",
    "    AnomaliesIndex,\n",
    "    MaxIndex, \n",
    "    calculate_anomalies\n",
    ")\n",
    "from PredictorsDrivers import (\n",
    "    Predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_indices = pd.read_csv(\"data/my_indices/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 177\n",
      "\n",
      "Verification Results:\n",
      "  IDENTICAL: 124\n",
      "  ALMOST_IDENTICAL: 0\n",
      "  DIFFERENT_VALUES: 4\n",
      "  SHAPE_MISMATCH: 0\n",
      "  COLUMN_NAMES_MISMATCH: 0\n",
      "  MISSING_FILE: 0\n",
      "  ERROR: 0\n",
      "  SELF: 49\n",
      "\n",
      "Problematic ID comparisons:\n",
      "  8334b687 -> fde0e327: DIFFERENT_VALUES\n",
      "  e19aa330 -> fde0e327: DIFFERENT_VALUES\n",
      "  f25567c1 -> 340e2882: DIFFERENT_VALUES\n",
      "  19496680 -> 340e2882: DIFFERENT_VALUES\n",
      "\n",
      "WARNING: Some dataframes are not identical to their preserved versions!\n",
      "This could indicate data inconsistency in duplicated IDs.\n",
      "Consider reviewing these files before proceeding.\n",
      "Found 216 parquet files to process\n",
      "Modified predictor_1b939ac5_1.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_10.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_11.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_12.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_2.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_3.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_4.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_5.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_6.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_7.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_8.parquet: renamed 24 columns\n",
      "Modified predictor_1b939ac5_9.parquet: renamed 24 columns\n",
      "Modified predictor_30ab9bad_1.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_10.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_11.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_12.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_2.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_3.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_4.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_5.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_6.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_7.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_8.parquet: renamed 12 columns\n",
      "Modified predictor_30ab9bad_9.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_1.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_10.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_11.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_12.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_2.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_3.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_4.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_5.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_6.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_7.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_8.parquet: renamed 12 columns\n",
      "Modified predictor_311dd366_9.parquet: renamed 12 columns\n",
      "Modified predictor_3832cbd6_1.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_10.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_11.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_12.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_2.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_3.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_4.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_5.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_6.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_7.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_8.parquet: renamed 8 columns\n",
      "Modified predictor_3832cbd6_9.parquet: renamed 8 columns\n",
      "Modified predictor_3df87a13_1.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_10.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_11.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_12.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_2.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_3.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_4.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_5.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_6.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_7.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_8.parquet: renamed 12 columns\n",
      "Modified predictor_3df87a13_9.parquet: renamed 12 columns\n",
      "Modified predictor_4d17ba1a_1.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_10.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_11.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_12.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_2.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_3.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_4.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_5.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_6.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_7.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_8.parquet: renamed 14 columns\n",
      "Modified predictor_4d17ba1a_9.parquet: renamed 14 columns\n",
      "Modified predictor_50a3f070_1.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_10.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_11.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_12.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_2.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_3.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_4.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_5.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_6.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_7.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_8.parquet: renamed 23 columns\n",
      "Modified predictor_50a3f070_9.parquet: renamed 23 columns\n",
      "Modified predictor_511854f2_1.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_10.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_11.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_12.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_2.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_3.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_4.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_5.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_6.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_7.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_8.parquet: renamed 14 columns\n",
      "Modified predictor_511854f2_9.parquet: renamed 14 columns\n",
      "Modified predictor_69ae08a8_1.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_10.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_11.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_12.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_2.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_3.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_4.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_5.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_6.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_7.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_8.parquet: renamed 23 columns\n",
      "Modified predictor_69ae08a8_9.parquet: renamed 23 columns\n",
      "Modified predictor_8c95fd00_1.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_10.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_11.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_12.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_2.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_3.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_4.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_5.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_6.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_7.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_8.parquet: renamed 12 columns\n",
      "Modified predictor_8c95fd00_9.parquet: renamed 12 columns\n",
      "Modified predictor_978f49d7_1.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_10.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_11.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_12.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_2.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_3.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_4.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_5.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_6.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_7.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_8.parquet: renamed 2 columns\n",
      "Modified predictor_978f49d7_9.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_1.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_10.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_11.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_12.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_2.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_3.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_4.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_5.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_6.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_7.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_8.parquet: renamed 2 columns\n",
      "Modified predictor_9bd58418_9.parquet: renamed 2 columns\n",
      "Modified predictor_b33fc639_1.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_10.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_11.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_12.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_2.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_3.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_4.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_5.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_6.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_7.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_8.parquet: renamed 14 columns\n",
      "Modified predictor_b33fc639_9.parquet: renamed 14 columns\n",
      "Modified predictor_d7101242_1.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_10.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_11.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_12.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_2.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_3.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_4.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_5.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_6.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_7.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_8.parquet: renamed 10 columns\n",
      "Modified predictor_d7101242_9.parquet: renamed 10 columns\n",
      "Modified predictor_e5b79485_1.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_10.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_11.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_12.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_2.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_3.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_4.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_5.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_6.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_7.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_8.parquet: renamed 23 columns\n",
      "Modified predictor_e5b79485_9.parquet: renamed 23 columns\n",
      "\n",
      "Summary:\n",
      "Total files processed: 216\n",
      "Files modified: 180\n",
      "Columns renamed: 2460\n",
      "\n",
      "Updating experiment metadata file: data/climate_features/chile/metadata.csv\n",
      "Original rows: 192\n",
      "Updated experiment metadata:\n",
      "  Modified rows: 0 out of 192\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def deduplicate_metadata(file_path, indices_dir=\"data/my_indices/\"):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Identify the columns to check for duplicates\n",
    "    duplicate_check_columns = [\n",
    "        'method', 'rolling', 'variables', 'boxes', \n",
    "        'reference_period', 'target_period'\n",
    "    ]\n",
    "    \n",
    "    # Print original number of rows\n",
    "    print(f\"Original number of rows: {len(df)}\")\n",
    "    \n",
    "    # Dictionary to store the mapping from unique combination to latest id\n",
    "    latest_id_by_key = {}\n",
    "    \n",
    "    # Dictionary to store all ids associated with each unique combination\n",
    "    all_ids_by_key = {}\n",
    "    \n",
    "    # Iterate through the dataframe rows in order\n",
    "    for index, row in df.iterrows():\n",
    "        # Create a tuple of the values in the duplicate check columns\n",
    "        key = tuple(row[duplicate_check_columns])\n",
    "        \n",
    "        # Store the id, overwriting any previous id with the same key\n",
    "        current_id = row['id']\n",
    "        \n",
    "        # Track all ids associated with this key\n",
    "        if key not in all_ids_by_key:\n",
    "            all_ids_by_key[key] = []\n",
    "        all_ids_by_key[key].append(current_id)\n",
    "        \n",
    "        # Update the latest id for this key\n",
    "        latest_id_by_key[key] = current_id\n",
    "    \n",
    "    # Create mapping dictionary from any id to its preserved id\n",
    "    id_mapping = {}\n",
    "    verification_results = {}\n",
    "    \n",
    "    for key, ids in all_ids_by_key.items():\n",
    "        preserved_id = latest_id_by_key[key]\n",
    "        \n",
    "        for id_val in ids:\n",
    "            id_mapping[id_val] = preserved_id\n",
    "            \n",
    "            # Skip verification if id is the preserved id (comparing with itself)\n",
    "            if id_val == preserved_id:\n",
    "                verification_results[id_val] = \"SELF\"\n",
    "                continue\n",
    "            \n",
    "            # Verify that corresponding dataframes are identical\n",
    "            try:\n",
    "                old_df_path = os.path.join(indices_dir, f\"index_{id_val}.parquet\")\n",
    "                preserved_df_path = os.path.join(indices_dir, f\"index_{preserved_id}.parquet\")\n",
    "                \n",
    "                if os.path.exists(old_df_path) and os.path.exists(preserved_df_path):\n",
    "                    old_df = pd.read_parquet(old_df_path)\n",
    "                    preserved_df = pd.read_parquet(preserved_df_path)\n",
    "                    \n",
    "                    # Convert to numpy arrays and check if they're equal\n",
    "                    old_array = old_df.values\n",
    "                    preserved_array = preserved_df.values\n",
    "                    \n",
    "                    # Check if shapes match\n",
    "                    if old_array.shape != preserved_array.shape:\n",
    "                        verification_results[id_val] = f\"SHAPE_MISMATCH: {old_array.shape} vs {preserved_array.shape}\"\n",
    "                    # Check if column names match\n",
    "                    elif not np.array_equal(old_df.columns, preserved_df.columns):\n",
    "                        verification_results[id_val] = \"COLUMN_NAMES_MISMATCH\"\n",
    "                    # Check if values are identical\n",
    "                    elif np.array_equal(old_array, preserved_array):\n",
    "                        verification_results[id_val] = \"IDENTICAL\"\n",
    "                    else:\n",
    "                        # Check for almost equal (floating point differences)\n",
    "                        if np.issubdtype(old_array.dtype, np.number) and np.issubdtype(preserved_array.dtype, np.number):\n",
    "                            if np.allclose(old_array, preserved_array, rtol=1e-5, atol=1e-8, equal_nan=True):\n",
    "                                verification_results[id_val] = \"ALMOST_IDENTICAL\"\n",
    "                            else:\n",
    "                                verification_results[id_val] = \"DIFFERENT_VALUES\"\n",
    "                        else:\n",
    "                            verification_results[id_val] = \"DIFFERENT_VALUES\"\n",
    "                else:\n",
    "                    if not os.path.exists(old_df_path):\n",
    "                        verification_results[id_val] = f\"MISSING_FILE: {old_df_path}\"\n",
    "                    elif not os.path.exists(preserved_df_path):\n",
    "                        verification_results[id_val] = f\"MISSING_FILE: {preserved_df_path}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                verification_results[id_val] = f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    # Print verification summary\n",
    "    print(\"\\nVerification Results:\")\n",
    "    status_counts = {\"IDENTICAL\": 0, \"ALMOST_IDENTICAL\": 0, \"DIFFERENT_VALUES\": 0, \n",
    "                     \"SHAPE_MISMATCH\": 0, \"COLUMN_NAMES_MISMATCH\": 0, \"MISSING_FILE\": 0, \n",
    "                     \"ERROR\": 0, \"SELF\": 0}\n",
    "    \n",
    "    for id_val, status in verification_results.items():\n",
    "        base_status = status.split(\":\")[0] if \":\" in status else status\n",
    "        status_counts[base_status] = status_counts.get(base_status, 0) + 1\n",
    "    \n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count}\")\n",
    "    \n",
    "    # List any problematic comparisons\n",
    "    problems = [id_val for id_val, status in verification_results.items() \n",
    "                if not (status == \"IDENTICAL\" or status == \"ALMOST_IDENTICAL\" or status == \"SELF\")]\n",
    "    \n",
    "    if problems:\n",
    "        print(\"\\nProblematic ID comparisons:\")\n",
    "        for id_val in problems[:10]:  # Limit to first 10 to avoid huge output\n",
    "            print(f\"  {id_val} -> {id_mapping[id_val]}: {verification_results[id_val]}\")\n",
    "        if len(problems) > 10:\n",
    "            print(f\"  ... and {len(problems) - 10} more\")\n",
    "    \n",
    "    # Assert that all comparisons are valid\n",
    "    invalid_comparisons = [id_val for id_val, status in verification_results.items() \n",
    "                          if not (status == \"IDENTICAL\" or status == \"ALMOST_IDENTICAL\" or status == \"SELF\")]\n",
    "    \n",
    "    if invalid_comparisons:\n",
    "        print(\"\\nWARNING: Some dataframes are not identical to their preserved versions!\")\n",
    "        print(\"This could indicate data inconsistency in duplicated IDs.\")\n",
    "        print(\"Consider reviewing these files before proceeding.\")\n",
    "    else:\n",
    "        print(\"\\nAll verification checks passed. Duplicated IDs reference identical data.\")\n",
    "    \n",
    "    return id_mapping, verification_results\n",
    "\n",
    "def rename_columns_in_parquet_files(directory_path, id_mapping):\n",
    "    # Regular expression to match the column name format \"{id}-{var}\"\n",
    "    pattern = re.compile(r\"([a-zA-Z0-9]+)-(.+)\")\n",
    "    \n",
    "    # Count variables for summary\n",
    "    total_files = 0\n",
    "    modified_files = 0\n",
    "    renamed_columns = 0\n",
    "    \n",
    "    # List all parquet files in the directory that match the pattern\n",
    "    parquet_files = [f for f in os.listdir(directory_path) \n",
    "                     if f.startswith(\"predictor_\") and f.endswith(\".parquet\")]\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} parquet files to process\")\n",
    "    \n",
    "    for file_name in parquet_files:\n",
    "        total_files += 1\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Read the parquet file\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            original_columns = df.columns.tolist()\n",
    "            \n",
    "            # Create new column names dictionary\n",
    "            column_mapping = {}\n",
    "            file_modified = False\n",
    "            \n",
    "            for column in original_columns:\n",
    "                match = pattern.match(column)\n",
    "                if match:\n",
    "                    old_id = match.group(1)\n",
    "                    var_part = match.group(2)\n",
    "                    \n",
    "                    # Check if this ID needs to be replaced\n",
    "                    if old_id in id_mapping and old_id != id_mapping[old_id]:\n",
    "                        new_column = f\"{id_mapping[old_id]}-{var_part}\"\n",
    "                        column_mapping[column] = new_column\n",
    "                        renamed_columns += 1\n",
    "                        file_modified = True\n",
    "            \n",
    "            # Rename columns if needed\n",
    "            if file_modified:\n",
    "                df = df.rename(columns=column_mapping)\n",
    "                modified_files += 1\n",
    "                \n",
    "                # Save the file with renamed columns\n",
    "                df.to_parquet(file_path, index=False)\n",
    "                \n",
    "                print(f\"Modified {file_name}: renamed {len(column_mapping)} columns\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Total files processed: {total_files}\")\n",
    "    print(f\"Files modified: {modified_files}\")\n",
    "    print(f\"Columns renamed: {renamed_columns}\")\n",
    "\n",
    "def update_experiment_metadata(experiment_metadata_path, id_mapping):\n",
    "    \"\"\"\n",
    "    Update the indices column in the experiment metadata file by replacing IDs with their preserved versions.\n",
    "    \n",
    "    Parameters:\n",
    "    experiment_metadata_path (str): Path to the experiment metadata CSV file\n",
    "    id_mapping (dict): Dictionary mapping from original IDs to preserved IDs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the experiment metadata CSV\n",
    "        exp_df = pd.read_csv(experiment_metadata_path)\n",
    "        \n",
    "        print(f\"\\nUpdating experiment metadata file: {experiment_metadata_path}\")\n",
    "        print(f\"Original rows: {len(exp_df)}\")\n",
    "        \n",
    "        # Function to replace IDs in the indices column\n",
    "        def replace_indices(indices_str):\n",
    "            # Split the indices string by hyphens\n",
    "            indices = indices_str.split('-')\n",
    "            \n",
    "            # Replace each index with its preserved version if it exists in the mapping\n",
    "            for i, index in enumerate(indices):\n",
    "                if index in id_mapping:\n",
    "                    indices[i] = id_mapping[index]\n",
    "            \n",
    "            # Join the indices back with hyphens\n",
    "            return '-'.join(indices)\n",
    "        \n",
    "        # Apply the function to the indices column\n",
    "        original_indices = exp_df['indices'].copy()\n",
    "        exp_df['indices'] = exp_df['indices'].apply(replace_indices)\n",
    "        \n",
    "        # Count how many rows were modified\n",
    "        modified_rows = sum(original_indices != exp_df['indices'])\n",
    "        \n",
    "        # Save the updated dataframe back to CSV\n",
    "        exp_df.to_csv(experiment_metadata_path, index=False)\n",
    "        \n",
    "        print(f\"Updated experiment metadata:\")\n",
    "        print(f\"  Modified rows: {modified_rows} out of {len(exp_df)}\")\n",
    "        \n",
    "        # Show a sample of modifications if any were made\n",
    "        if modified_rows > 0:\n",
    "            print(\"\\nSample of changes (first 3):\")\n",
    "            changes = 0\n",
    "            for i, (orig, new) in enumerate(zip(original_indices, exp_df['indices'])):\n",
    "                if orig != new:\n",
    "                    print(f\"  Row {i}:\")\n",
    "                    print(f\"    Original: {orig}\")\n",
    "                    print(f\"    Updated:  {new}\")\n",
    "                    changes += 1\n",
    "                    if changes >= 3:\n",
    "                        break\n",
    "            \n",
    "        return modified_rows\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating experiment metadata: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# File paths\n",
    "indices_metadata_file = \"data/my_indices/metadata.csv\"  # Path to your indices metadata CSV file\n",
    "indices_directory = \"data/my_indices/\"  # Directory containing index parquet files\n",
    "parquet_directory = \"data/climate_features/chile\"  # Directory for predictor parquet files\n",
    "experiment_metadata_file = \"data/climate_features/chile/metadata.csv\"  # Experiment metadata\n",
    "\n",
    "# Get the ID mapping and verify data integrity\n",
    "id_mapping, verification_results = deduplicate_metadata(indices_metadata_file, indices_directory)\n",
    "\n",
    "# Flag to track if verification passed\n",
    "verification_passed = not any(status not in [\"IDENTICAL\", \"ALMOST_IDENTICAL\", \"SELF\"] \n",
    "                                for status in verification_results.values())\n",
    "\n",
    "if verification_passed or input(\"\\nProceed despite verification issues? (y/n): \").lower() == 'y':\n",
    "    # Process all parquet files\n",
    "    rename_columns_in_parquet_files(parquet_directory, id_mapping)\n",
    "    \n",
    "    # Update experiment metadata file\n",
    "    if os.path.exists(experiment_metadata_file):\n",
    "        update_experiment_metadata(experiment_metadata_file, id_mapping)\n",
    "    else:\n",
    "        print(f\"\\nExperiment metadata file not found: {experiment_metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
