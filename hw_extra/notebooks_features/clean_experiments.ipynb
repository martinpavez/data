{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from metpy.calc import advection\n",
    "from metpy.units import units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Desktop\\data\\hw_extra\n"
     ]
    }
   ],
   "source": [
    "# Add the folder to the Python path\n",
    "\n",
    "os.chdir(\"../\")\n",
    "# change working directory to project's root path\n",
    "print(os.getcwd())\n",
    "\n",
    "folder_path = os.path.abspath(\"functions/\") #INPUT_PATH)#'path_to_your_folder')  # Replace with the actual folder path\n",
    "sys.path.insert(0, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndexDrivers import (\n",
    "    AnomaliesIndex,\n",
    "    MaxIndex, \n",
    "    calculate_anomalies\n",
    ")\n",
    "from PredictorsDrivers import (\n",
    "    Predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_indices = pd.read_csv(\"data/my_indices/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 177\n",
      "\n",
      "Verification Results:\n",
      "  IDENTICAL: 124\n",
      "  ALMOST_IDENTICAL: 0\n",
      "  DIFFERENT_VALUES: 4\n",
      "  SHAPE_MISMATCH: 0\n",
      "  COLUMN_NAMES_MISMATCH: 0\n",
      "  MISSING_FILE: 0\n",
      "  ERROR: 0\n",
      "  SELF: 49\n",
      "\n",
      "Problematic ID comparisons:\n",
      "  8334b687 -> fde0e327: DIFFERENT_VALUES\n",
      "  e19aa330 -> fde0e327: DIFFERENT_VALUES\n",
      "  f25567c1 -> 340e2882: DIFFERENT_VALUES\n",
      "  19496680 -> 340e2882: DIFFERENT_VALUES\n",
      "\n",
      "WARNING: Some dataframes are not identical to their preserved versions!\n",
      "This could indicate data inconsistency in duplicated IDs.\n",
      "Consider reviewing these files before proceeding.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def deduplicate_metadata(file_path, indices_dir=\"data/my_indices/\"):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Identify the columns to check for duplicates\n",
    "    duplicate_check_columns = [\n",
    "        'method', 'rolling', 'variables', 'boxes', \n",
    "        'reference_period', 'target_period'\n",
    "    ]\n",
    "    \n",
    "    # Print original number of rows\n",
    "    print(f\"Original number of rows: {len(df)}\")\n",
    "    \n",
    "    # Dictionary to store the mapping from unique combination to latest id\n",
    "    latest_id_by_key = {}\n",
    "    \n",
    "    # Dictionary to store all ids associated with each unique combination\n",
    "    all_ids_by_key = {}\n",
    "    \n",
    "    # Iterate through the dataframe rows in order\n",
    "    for index, row in df.iterrows():\n",
    "        # Create a tuple of the values in the duplicate check columns\n",
    "        key = tuple(row[duplicate_check_columns])\n",
    "        \n",
    "        # Store the id, overwriting any previous id with the same key\n",
    "        current_id = row['id']\n",
    "        \n",
    "        # Track all ids associated with this key\n",
    "        if key not in all_ids_by_key:\n",
    "            all_ids_by_key[key] = []\n",
    "        all_ids_by_key[key].append(current_id)\n",
    "        \n",
    "        # Update the latest id for this key\n",
    "        latest_id_by_key[key] = current_id\n",
    "    \n",
    "    # Create mapping dictionary from any id to its preserved id\n",
    "    id_mapping = {}\n",
    "    verification_results = {}\n",
    "    \n",
    "    for key, ids in all_ids_by_key.items():\n",
    "        preserved_id = latest_id_by_key[key]\n",
    "        \n",
    "        for id_val in ids:\n",
    "            id_mapping[id_val] = preserved_id\n",
    "            \n",
    "            # Skip verification if id is the preserved id (comparing with itself)\n",
    "            if id_val == preserved_id:\n",
    "                verification_results[id_val] = \"SELF\"\n",
    "                continue\n",
    "            \n",
    "            # Verify that corresponding dataframes are identical\n",
    "            try:\n",
    "                old_df_path = os.path.join(indices_dir, f\"index_{id_val}.parquet\")\n",
    "                preserved_df_path = os.path.join(indices_dir, f\"index_{preserved_id}.parquet\")\n",
    "                \n",
    "                if os.path.exists(old_df_path) and os.path.exists(preserved_df_path):\n",
    "                    old_df = pd.read_parquet(old_df_path)\n",
    "                    preserved_df = pd.read_parquet(preserved_df_path)\n",
    "                    \n",
    "                    # Convert to numpy arrays and check if they're equal\n",
    "                    old_array = old_df.values\n",
    "                    preserved_array = preserved_df.values\n",
    "                    \n",
    "                    # Check if shapes match\n",
    "                    if old_array.shape != preserved_array.shape:\n",
    "                        verification_results[id_val] = f\"SHAPE_MISMATCH: {old_array.shape} vs {preserved_array.shape}\"\n",
    "                    # Check if column names match\n",
    "                    elif not np.array_equal(old_df.columns, preserved_df.columns):\n",
    "                        verification_results[id_val] = \"COLUMN_NAMES_MISMATCH\"\n",
    "                    # Check if values are identical\n",
    "                    elif np.array_equal(old_array, preserved_array):\n",
    "                        verification_results[id_val] = \"IDENTICAL\"\n",
    "                    else:\n",
    "                        # Check for almost equal (floating point differences)\n",
    "                        if np.issubdtype(old_array.dtype, np.number) and np.issubdtype(preserved_array.dtype, np.number):\n",
    "                            if np.allclose(old_array, preserved_array, rtol=1e-5, atol=1e-8, equal_nan=True):\n",
    "                                verification_results[id_val] = \"ALMOST_IDENTICAL\"\n",
    "                            else:\n",
    "                                verification_results[id_val] = \"DIFFERENT_VALUES\"\n",
    "                        else:\n",
    "                            verification_results[id_val] = \"DIFFERENT_VALUES\"\n",
    "                else:\n",
    "                    if not os.path.exists(old_df_path):\n",
    "                        verification_results[id_val] = f\"MISSING_FILE: {old_df_path}\"\n",
    "                    elif not os.path.exists(preserved_df_path):\n",
    "                        verification_results[id_val] = f\"MISSING_FILE: {preserved_df_path}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                verification_results[id_val] = f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    # Print verification summary\n",
    "    print(\"\\nVerification Results:\")\n",
    "    status_counts = {\"IDENTICAL\": 0, \"ALMOST_IDENTICAL\": 0, \"DIFFERENT_VALUES\": 0, \n",
    "                     \"SHAPE_MISMATCH\": 0, \"COLUMN_NAMES_MISMATCH\": 0, \"MISSING_FILE\": 0, \n",
    "                     \"ERROR\": 0, \"SELF\": 0}\n",
    "    \n",
    "    for id_val, status in verification_results.items():\n",
    "        base_status = status.split(\":\")[0] if \":\" in status else status\n",
    "        status_counts[base_status] = status_counts.get(base_status, 0) + 1\n",
    "    \n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count}\")\n",
    "    \n",
    "    # List any problematic comparisons\n",
    "    problems = [id_val for id_val, status in verification_results.items() \n",
    "                if not (status == \"IDENTICAL\" or status == \"ALMOST_IDENTICAL\" or status == \"SELF\")]\n",
    "    \n",
    "    if problems:\n",
    "        print(\"\\nProblematic ID comparisons:\")\n",
    "        for id_val in problems[:10]:  # Limit to first 10 to avoid huge output\n",
    "            print(f\"  {id_val} -> {id_mapping[id_val]}: {verification_results[id_val]}\")\n",
    "        if len(problems) > 10:\n",
    "            print(f\"  ... and {len(problems) - 10} more\")\n",
    "    \n",
    "    # Assert that all comparisons are valid\n",
    "    invalid_comparisons = [id_val for id_val, status in verification_results.items() \n",
    "                          if not (status == \"IDENTICAL\" or status == \"ALMOST_IDENTICAL\" or status == \"SELF\")]\n",
    "    \n",
    "    if invalid_comparisons:\n",
    "        print(\"\\nWARNING: Some dataframes are not identical to their preserved versions!\")\n",
    "        print(\"This could indicate data inconsistency in duplicated IDs.\")\n",
    "        print(\"Consider reviewing these files before proceeding.\")\n",
    "    else:\n",
    "        print(\"\\nAll verification checks passed. Duplicated IDs reference identical data.\")\n",
    "    \n",
    "    return id_mapping, verification_results\n",
    "\n",
    "def rename_columns_in_parquet_files(directory_path, id_mapping):\n",
    "    # Regular expression to match the column name format \"{id}-{var}\"\n",
    "    pattern = re.compile(r\"([a-zA-Z0-9]+)-(.+)\")\n",
    "    \n",
    "    # Count variables for summary\n",
    "    total_files = 0\n",
    "    modified_files = 0\n",
    "    renamed_columns = 0\n",
    "    \n",
    "    # List all parquet files in the directory that match the pattern\n",
    "    parquet_files = [f for f in os.listdir(directory_path) \n",
    "                     if f.startswith(\"predictor_\") and f.endswith(\".parquet\")]\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} parquet files to process\")\n",
    "    \n",
    "    for file_name in parquet_files:\n",
    "        total_files += 1\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Read the parquet file\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            original_columns = df.columns.tolist()\n",
    "            \n",
    "            # Create new column names dictionary\n",
    "            column_mapping = {}\n",
    "            file_modified = False\n",
    "            \n",
    "            for column in original_columns:\n",
    "                match = pattern.match(column)\n",
    "                if match:\n",
    "                    old_id = match.group(1)\n",
    "                    var_part = match.group(2)\n",
    "                    \n",
    "                    # Check if this ID needs to be replaced\n",
    "                    if old_id in id_mapping and old_id != id_mapping[old_id]:\n",
    "                        new_column = f\"{id_mapping[old_id]}-{var_part}\"\n",
    "                        column_mapping[column] = new_column\n",
    "                        renamed_columns += 1\n",
    "                        file_modified = True\n",
    "            \n",
    "            # Rename columns if needed\n",
    "            if file_modified:\n",
    "                df = df.rename(columns=column_mapping)\n",
    "                modified_files += 1\n",
    "                \n",
    "                # Save the file with renamed columns\n",
    "                df.to_parquet(file_path, index=False)\n",
    "                \n",
    "                print(f\"Modified {file_name}: renamed {len(column_mapping)} columns\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Total files processed: {total_files}\")\n",
    "    print(f\"Files modified: {modified_files}\")\n",
    "    print(f\"Columns renamed: {renamed_columns}\")\n",
    "\n",
    "metadata_file = \"data/my_indices/metadata.csv\"  # Path to your metadata CSV file\n",
    "parquet_directory = \"data/climate_features/chile\"  # Directory containing parquet files\n",
    "indices_directory = \"data/my_indices/\"  # Directory containing index parquet files\n",
    "\n",
    "# Get the ID mapping and verify data integrity\n",
    "id_mapping, verification_results = deduplicate_metadata(metadata_file, indices_directory)\n",
    "\n",
    "#Process all parquet files if verification passed\n",
    "if not any(status not in [\"IDENTICAL\", \"ALMOST_IDENTICAL\", \"SELF\"] \n",
    "            for status in verification_results.values()):\n",
    "    rename_columns_in_parquet_files(parquet_directory, id_mapping)\n",
    "else:\n",
    "    print(\"\\nColumn renaming skipped due to verification issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet(\"data/my_indices/index_8334b687.parquet\")\n",
    "df2 = pd.read_parquet(\"data/my_indices/index_e19aa330.parquet\")\n",
    "df3 = pd.read_parquet(\"data/my_indices/index_fde0e327.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sst</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1972-01-31</th>\n",
       "      <td>-0.766016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-02-29</th>\n",
       "      <td>-0.183218</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-03-31</th>\n",
       "      <td>-0.156800</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-04-30</th>\n",
       "      <td>0.586059</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-05-31</th>\n",
       "      <td>0.997811</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31</th>\n",
       "      <td>-0.698484</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-30</th>\n",
       "      <td>-0.830900</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-31</th>\n",
       "      <td>-0.817449</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30</th>\n",
       "      <td>-0.872172</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>-0.764503</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sst  month\n",
       "time                       \n",
       "1972-01-31 -0.766016      1\n",
       "1972-02-29 -0.183218      2\n",
       "1972-03-31 -0.156800      3\n",
       "1972-04-30  0.586059      4\n",
       "1972-05-31  0.997811      5\n",
       "...              ...    ...\n",
       "2022-08-31 -0.698484      8\n",
       "2022-09-30 -0.830900      9\n",
       "2022-10-31 -0.817449     10\n",
       "2022-11-30 -0.872172     11\n",
       "2022-12-31 -0.764503     12\n",
       "\n",
       "[612 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sst</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1972-01-31</th>\n",
       "      <td>-0.766016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-02-29</th>\n",
       "      <td>-0.183218</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-03-31</th>\n",
       "      <td>-0.156800</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-04-30</th>\n",
       "      <td>0.586059</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-05-31</th>\n",
       "      <td>0.997811</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31</th>\n",
       "      <td>-0.698484</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-30</th>\n",
       "      <td>-0.830900</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-31</th>\n",
       "      <td>-0.817449</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30</th>\n",
       "      <td>-0.872172</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>-0.764503</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sst  month\n",
       "time                       \n",
       "1972-01-31 -0.766016      1\n",
       "1972-02-29 -0.183218      2\n",
       "1972-03-31 -0.156800      3\n",
       "1972-04-30  0.586059      4\n",
       "1972-05-31  0.997811      5\n",
       "...              ...    ...\n",
       "2022-08-31 -0.698484      8\n",
       "2022-09-30 -0.830900      9\n",
       "2022-10-31 -0.817449     10\n",
       "2022-11-30 -0.872172     11\n",
       "2022-12-31 -0.764503     12\n",
       "\n",
       "[612 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sst</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1972-01-31</th>\n",
       "      <td>-0.823776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-02-29</th>\n",
       "      <td>-0.209746</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-03-31</th>\n",
       "      <td>-0.145999</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-04-30</th>\n",
       "      <td>0.634867</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972-05-31</th>\n",
       "      <td>1.080823</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31</th>\n",
       "      <td>-0.724015</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-30</th>\n",
       "      <td>-0.849793</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-31</th>\n",
       "      <td>-0.810017</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30</th>\n",
       "      <td>-0.910607</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>-0.777572</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sst  month\n",
       "time                       \n",
       "1972-01-31 -0.823776      1\n",
       "1972-02-29 -0.209746      2\n",
       "1972-03-31 -0.145999      3\n",
       "1972-04-30  0.634867      4\n",
       "1972-05-31  1.080823      5\n",
       "...              ...    ...\n",
       "2022-08-31 -0.724015      8\n",
       "2022-09-30 -0.849793      9\n",
       "2022-10-31 -0.810017     10\n",
       "2022-11-30 -0.910607     11\n",
       "2022-12-31 -0.777572     12\n",
       "\n",
       "[612 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df1)\n",
    "display(df2)\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
