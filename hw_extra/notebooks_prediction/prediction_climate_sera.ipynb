{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SERA Monthly Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "SEED = 42\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    keras.utils.set_random_seed(seed)\n",
    "\n",
    "def set_global_determinism(seed=SEED):\n",
    "    set_seeds(seed=seed)\n",
    "\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout, GRU, Conv1D, Flatten, Reshape\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import sys\n",
    "from catboost import CatBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Desktop\\data\\hw_extra\n"
     ]
    }
   ],
   "source": [
    "# Add the folder to the Python path\n",
    "\n",
    "os.chdir(\"../\")\n",
    "# change working directory to project's root path\n",
    "print(os.getcwd())\n",
    "\n",
    "FIRST_YEAR= 1972\n",
    "FREQUENCY= \"monthly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.abspath(\"functions/\") #INPUT_PATH)#'path_to_your_folder')  # Replace with the actual folder path\n",
    "sys.path.insert(0, folder_path)\n",
    "\n",
    "from Predictions import (\n",
    "    PredictionExperiment,\n",
    "    PredictionModel,\n",
    "    SERA,\n",
    "    sera_objective,\n",
    "    piecewise_linear_phi_np\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_interest = [\"HWN\", \"HWF\", \"HWD\", \"HWM\", \"HWA\"]\n",
    "bounds_v1 = (-1.1692892810242344, -0.30647585455315646, 4.561547586528888, 6.499969486244418)\n",
    "bounds = (-1.1692892810242344, -0.30647585455315646, 3.0, 6.499969486244418)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>season</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_1.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_2.parquet</td>\n",
       "      <td>2</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_3.parquet</td>\n",
       "      <td>3</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_4.parquet</td>\n",
       "      <td>4</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_5.parquet</td>\n",
       "      <td>5</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_6.parquet</td>\n",
       "      <td>6</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_7.parquet</td>\n",
       "      <td>7</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_8.parquet</td>\n",
       "      <td>8</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_9.parquet</td>\n",
       "      <td>9</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_10.parquet</td>\n",
       "      <td>10</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_11.parquet</td>\n",
       "      <td>11</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6e47cb06</td>\n",
       "      <td>predictor_6e47cb06_12.parquet</td>\n",
       "      <td>12</td>\n",
       "      <td>df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_1.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_2.parquet</td>\n",
       "      <td>2</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_3.parquet</td>\n",
       "      <td>3</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_4.parquet</td>\n",
       "      <td>4</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_5.parquet</td>\n",
       "      <td>5</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_6.parquet</td>\n",
       "      <td>6</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_7.parquet</td>\n",
       "      <td>7</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_8.parquet</td>\n",
       "      <td>8</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_9.parquet</td>\n",
       "      <td>9</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_10.parquet</td>\n",
       "      <td>10</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_11.parquet</td>\n",
       "      <td>11</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5cb3fa02</td>\n",
       "      <td>predictor_5cb3fa02_12.parquet</td>\n",
       "      <td>12</td>\n",
       "      <td>fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                       filename  season   \n",
       "0   6e47cb06   predictor_6e47cb06_1.parquet       1  \\\n",
       "1   6e47cb06   predictor_6e47cb06_2.parquet       2   \n",
       "2   6e47cb06   predictor_6e47cb06_3.parquet       3   \n",
       "3   6e47cb06   predictor_6e47cb06_4.parquet       4   \n",
       "4   6e47cb06   predictor_6e47cb06_5.parquet       5   \n",
       "5   6e47cb06   predictor_6e47cb06_6.parquet       6   \n",
       "6   6e47cb06   predictor_6e47cb06_7.parquet       7   \n",
       "7   6e47cb06   predictor_6e47cb06_8.parquet       8   \n",
       "8   6e47cb06   predictor_6e47cb06_9.parquet       9   \n",
       "9   6e47cb06  predictor_6e47cb06_10.parquet      10   \n",
       "10  6e47cb06  predictor_6e47cb06_11.parquet      11   \n",
       "11  6e47cb06  predictor_6e47cb06_12.parquet      12   \n",
       "12  5cb3fa02   predictor_5cb3fa02_1.parquet       1   \n",
       "13  5cb3fa02   predictor_5cb3fa02_2.parquet       2   \n",
       "14  5cb3fa02   predictor_5cb3fa02_3.parquet       3   \n",
       "15  5cb3fa02   predictor_5cb3fa02_4.parquet       4   \n",
       "16  5cb3fa02   predictor_5cb3fa02_5.parquet       5   \n",
       "17  5cb3fa02   predictor_5cb3fa02_6.parquet       6   \n",
       "18  5cb3fa02   predictor_5cb3fa02_7.parquet       7   \n",
       "19  5cb3fa02   predictor_5cb3fa02_8.parquet       8   \n",
       "20  5cb3fa02   predictor_5cb3fa02_9.parquet       9   \n",
       "21  5cb3fa02  predictor_5cb3fa02_10.parquet      10   \n",
       "22  5cb3fa02  predictor_5cb3fa02_11.parquet      11   \n",
       "23  5cb3fa02  predictor_5cb3fa02_12.parquet      12   \n",
       "\n",
       "                                              indices  \n",
       "0   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "1   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "2   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "3   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "4   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "5   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "6   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "7   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "8   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "9   df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "10  df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "11  df9a31c5-20a07cea-cfb03125-9169e0dc-0b0bffae-b...  \n",
       "12  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "13  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "14  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "15  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "16  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "17  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "18  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "19  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "20  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "21  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "22  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  \n",
       "23  fde0e327-340e2882-f27c56aa-5b9237bf-46fa0cb8-6...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "region=\"california\"\n",
    "metadata_path = f\"data/climate_features/{region}/metadata.csv\"\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.reset_index(inplace=True, drop=True)\n",
    "display(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f\"data/sera_results/{region}_results/results.csv\")\n",
    "ids_results = results[\"id_data\"].unique()\n",
    "id_experiments = metadata[\"id\"].unique()\n",
    "ids_to_execute = [id for id in id_experiments if id not in ids_results]\n",
    "print(len(ids_to_execute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import xgboost as xgb\n",
    "class XGBCustomObjective(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Wrapper class to make XGBoost with custom objectives compatible with sklearn.\n",
    "    This is needed for MultiOutputRegressor.\n",
    "    \"\"\"\n",
    "    def __init__(self, objective_func=None, random_state=42, n_estimators=15, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, **kwargs):\n",
    "        self.objective_func = objective_func\n",
    "        self.random_state = random_state\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.kwargs = kwargs\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model with custom objective.\"\"\"\n",
    "        # Create DMatrix\n",
    "        dtrain = xgb.DMatrix(X, label=y)\n",
    "        \n",
    "        # Set up parameters using your specified values\n",
    "        params = {\n",
    "            'max_depth': self.max_depth,\n",
    "            'eta': self.learning_rate,  # learning_rate\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'seed': self.random_state,  # random_state\n",
    "            'disable_default_eval_metric': 1 if self.objective_func else 0\n",
    "        }\n",
    "        params.update(self.kwargs)\n",
    "        \n",
    "        if self.objective_func:\n",
    "            # Train with custom objective\n",
    "            self.model = xgb.train(\n",
    "                params=params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=self.n_estimators,\n",
    "                obj=self.objective_func,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "        else:\n",
    "            # Train with standard objective\n",
    "            params['objective'] = 'reg:squarederror'\n",
    "            del params['disable_default_eval_metric']\n",
    "            self.model = xgb.train(\n",
    "                params=params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=self.n_estimators,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        return self.model.predict(dtest)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator (required for sklearn compatibility).\"\"\"\n",
    "        params = {\n",
    "            'objective_func': self.objective_func,\n",
    "            'random_state': self.random_state,\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree\n",
    "        }\n",
    "        params.update(self.kwargs)\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters for this estimator (required for sklearn compatibility).\"\"\"\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                self.kwargs[key] = value\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for id in ids_to_execute:\n",
    "    k+=1\n",
    "    print(\"Executing\",id, \"iter\", k)\n",
    "    data = {i: pd.read_parquet(f\"data/climate_features/{region}/predictor_{id}_{i}.parquet\") for i in range(1,13)}\n",
    "    rnn16_model = Sequential([\n",
    "    SimpleRNN(16, activation=\"tanh\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "    Dropout(0.1),  # Regularization\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(len(indices_of_interest))  # Predict 5 indices\n",
    "    ])\n",
    "    lstm16_model = Sequential([\n",
    "    LSTM(16, activation=\"tanh\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "    Dropout(0.1),  # Regularization\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(len(indices_of_interest))  # Predict 5 indices\n",
    "    ])\n",
    "    cnn_rnn_model = Sequential([\n",
    "        Conv1D(16, kernel_size=1, activation=\"relu\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "        Reshape((1, 16)),  # Back to time dimension\n",
    "        SimpleRNN(8, activation=\"tanh\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(len(indices_of_interest))\n",
    "    ])\n",
    "    lp_model = Sequential([\n",
    "        Flatten(input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(len(indices_of_interest))\n",
    "    ])\n",
    "    xgb_model = XGBCustomObjective(\n",
    "        objective_func=sera_objective(piecewise_linear_phi_np(bounds, initial_weight=0.3)),\n",
    "        n_estimators=15,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    # assert len(regressors) == len(name_regressors)\n",
    "    regressors =  [xgb_model, rnn16_model, lstm16_model, cnn_rnn_model, lp_model]\n",
    "    name_regressors =  [\"CXGB15\", \"RNN16\", \"LSTM16\", \"CNNRNN16\", \"MLP16\"]\n",
    "    assert len(regressors) == len(name_regressors)\n",
    "    experiment_1 = PredictionExperiment(data, indices_of_interest, regressors, name_regressors, 5, id, loss_fn=SERA(bounds=bounds,T=100, initial_weight=0.3, fn=\"piecewise2\"))\n",
    "    experiment_1.execute_experiment()\n",
    "    experiment_1.get_metrics(\"r2\", \"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", \"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"TSCV\", show=False)\n",
    "\n",
    "    #experiment_1.top_results(\"r2\", 5, stage=\"prediction\", top_data_path=f\"data/results/{FREQUENCY}/top_results.csv\")\n",
    "    #experiment_1.top_results(\"cv_r2\", 5, stage=\"CV\", top_data_path=f\"data/results/{FREQUENCY}/top_results.csv\")\n",
    "    experiment_1.save_results(f\"data/sera_results_v2/{region}_results/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for id in ids_to_execute:\n",
    "    k+=1\n",
    "    print(\"Executing\",id, \"iter\", k)\n",
    "    data = {i: pd.read_parquet(f\"data/climate_features/{region}/predictor_{id}_{i}.parquet\") for i in range(1,13)}\n",
    "    rnn16_model = Sequential([\n",
    "    SimpleRNN(16, activation=\"tanh\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "    Dropout(0.1),  # Regularization\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(len(indices_of_interest))  # Predict 5 indices\n",
    "    ])\n",
    "    lstm16_model = Sequential([\n",
    "    LSTM(16, activation=\"tanh\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "    Dropout(0.1),  # Regularization\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(len(indices_of_interest))  # Predict 5 indices\n",
    "    ])\n",
    "    cnn_rnn_model = Sequential([\n",
    "        Conv1D(16, kernel_size=1, activation=\"relu\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "        Reshape((1, 16)),  # Back to time dimension\n",
    "        SimpleRNN(8, activation=\"tanh\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(len(indices_of_interest))\n",
    "    ])\n",
    "    lp_model = Sequential([\n",
    "        Flatten(input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(len(indices_of_interest))\n",
    "    ])\n",
    "    xgb_model = XGBCustomObjective(\n",
    "        objective_func=sera_objective(piecewise_linear_phi_np(bounds, initial_weight=0.3)),\n",
    "        n_estimators=15,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    # assert len(regressors) == len(name_regressors)\n",
    "    regressors =  [xgb_model, rnn16_model, lstm16_model, cnn_rnn_model, lp_model]\n",
    "    name_regressors =  [\"CXGB15\", \"RNN16\", \"LSTM16\", \"CNNRNN16\", \"MLP16\"]\n",
    "    assert len(regressors) == len(name_regressors)\n",
    "    experiment_1 = PredictionExperiment(data, indices_of_interest, regressors, name_regressors, 5, id, loss_fn=SERA(bounds=bounds,T=100, initial_weight=0.1, fn=\"piecewise2\"))\n",
    "    experiment_1.execute_experiment()\n",
    "    experiment_1.get_metrics(\"r2\", \"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", \"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"TSCV\", show=False)\n",
    "\n",
    "    #experiment_1.top_results(\"r2\", 5, stage=\"prediction\", top_data_path=f\"data/results/{FREQUENCY}/top_results.csv\")\n",
    "    #experiment_1.top_results(\"cv_r2\", 5, stage=\"CV\", top_data_path=f\"data/results/{FREQUENCY}/top_results.csv\")\n",
    "    experiment_1.save_results(f\"data/sera_results_v2/{region}_results/results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>season</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>978f49d7</td>\n",
       "      <td>predictor_978f49d7_1.parquet</td>\n",
       "      <td>1</td>\n",
       "      <td>fde0e327-340e2882-43701738-e306f58b-e601b072-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>978f49d7</td>\n",
       "      <td>predictor_978f49d7_2.parquet</td>\n",
       "      <td>2</td>\n",
       "      <td>fde0e327-340e2882-43701738-e306f58b-e601b072-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>978f49d7</td>\n",
       "      <td>predictor_978f49d7_3.parquet</td>\n",
       "      <td>3</td>\n",
       "      <td>fde0e327-340e2882-43701738-e306f58b-e601b072-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>978f49d7</td>\n",
       "      <td>predictor_978f49d7_4.parquet</td>\n",
       "      <td>4</td>\n",
       "      <td>fde0e327-340e2882-43701738-e306f58b-e601b072-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>978f49d7</td>\n",
       "      <td>predictor_978f49d7_5.parquet</td>\n",
       "      <td>5</td>\n",
       "      <td>fde0e327-340e2882-43701738-e306f58b-e601b072-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>458d357c</td>\n",
       "      <td>predictor_458d357c_8.parquet</td>\n",
       "      <td>8</td>\n",
       "      <td>32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>458d357c</td>\n",
       "      <td>predictor_458d357c_9.parquet</td>\n",
       "      <td>9</td>\n",
       "      <td>32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>458d357c</td>\n",
       "      <td>predictor_458d357c_10.parquet</td>\n",
       "      <td>10</td>\n",
       "      <td>32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>458d357c</td>\n",
       "      <td>predictor_458d357c_11.parquet</td>\n",
       "      <td>11</td>\n",
       "      <td>32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>458d357c</td>\n",
       "      <td>predictor_458d357c_12.parquet</td>\n",
       "      <td>12</td>\n",
       "      <td>32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                       filename  season   \n",
       "0    978f49d7   predictor_978f49d7_1.parquet       1  \\\n",
       "1    978f49d7   predictor_978f49d7_2.parquet       2   \n",
       "2    978f49d7   predictor_978f49d7_3.parquet       3   \n",
       "3    978f49d7   predictor_978f49d7_4.parquet       4   \n",
       "4    978f49d7   predictor_978f49d7_5.parquet       5   \n",
       "..        ...                            ...     ...   \n",
       "247  458d357c   predictor_458d357c_8.parquet       8   \n",
       "248  458d357c   predictor_458d357c_9.parquet       9   \n",
       "249  458d357c  predictor_458d357c_10.parquet      10   \n",
       "250  458d357c  predictor_458d357c_11.parquet      11   \n",
       "251  458d357c  predictor_458d357c_12.parquet      12   \n",
       "\n",
       "                                               indices  \n",
       "0    fde0e327-340e2882-43701738-e306f58b-e601b072-e...  \n",
       "1    fde0e327-340e2882-43701738-e306f58b-e601b072-e...  \n",
       "2    fde0e327-340e2882-43701738-e306f58b-e601b072-e...  \n",
       "3    fde0e327-340e2882-43701738-e306f58b-e601b072-e...  \n",
       "4    fde0e327-340e2882-43701738-e306f58b-e601b072-e...  \n",
       "..                                                 ...  \n",
       "247  32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...  \n",
       "248  32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...  \n",
       "249  32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...  \n",
       "250  32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...  \n",
       "251  32f131d2-69ffcfa8-4af95abb-4a86cb22-52eda853-3...  \n",
       "\n",
       "[252 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "region=\"chile\"\n",
    "metadata = pd.read_csv(f\"data/climate_features/{region}/metadata.csv\")\n",
    "metadata.reset_index(inplace=True, drop=True)\n",
    "display(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f\"data/sera_results_v2/{region}_results/results.csv\")\n",
    "ids_results = results[\"id_data\"].unique()\n",
    "id_experiments = metadata[\"id\"].unique()\n",
    "ids_to_execute = [id for id in id_experiments if id not in ids_results]\n",
    "print(len(ids_to_execute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 4d17ba1a iter 1\n",
      "Train predicting  1 CXGB15\n",
      "Train predicting  2 CXGB15\n",
      "Train predicting  3 CXGB15\n",
      "Train predicting  4 CXGB15\n",
      "Train predicting  5 CXGB15\n",
      "Train predicting  6 CXGB15\n",
      "Train predicting  7 CXGB15\n",
      "Train predicting  8 CXGB15\n",
      "Train predicting  9 CXGB15\n",
      "Train predicting  10 CXGB15\n",
      "Train predicting  11 CXGB15\n",
      "Train predicting  12 CXGB15\n",
      "Executing 3adff093 iter 2\n",
      "Train predicting  1 CXGB15\n",
      "Train predicting  2 CXGB15\n",
      "Train predicting  3 CXGB15\n",
      "Train predicting  4 CXGB15\n",
      "Train predicting  5 CXGB15\n",
      "Train predicting  6 CXGB15\n",
      "Train predicting  7 CXGB15\n",
      "Train predicting  8 CXGB15\n",
      "Train predicting  9 CXGB15\n",
      "Train predicting  10 CXGB15\n",
      "Train predicting  11 CXGB15\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for id in ids_to_execute:\n",
    "    k+=1\n",
    "    print(\"Executing\",id, \"iter\", k)\n",
    "    data = {i: pd.read_parquet(f\"data/climate_features/{region}/predictor_{id}_{i}.parquet\") for i in range(1,13)}\n",
    "    rnn16_model = Sequential([\n",
    "    SimpleRNN(16, activation=\"tanh\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "    Dropout(0.1),  # Regularization\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(len(indices_of_interest))  # Predict 5 indices\n",
    "    ])\n",
    "    lstm16_model = Sequential([\n",
    "    LSTM(16, activation=\"tanh\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "    Dropout(0.1),  # Regularization\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(len(indices_of_interest))  # Predict 5 indices\n",
    "    ])\n",
    "    cnn_rnn_model = Sequential([\n",
    "        Conv1D(16, kernel_size=1, activation=\"relu\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "        Reshape((1, 16)),  # Back to time dimension\n",
    "        SimpleRNN(8, activation=\"tanh\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(len(indices_of_interest))\n",
    "    ])\n",
    "    lp_model = Sequential([\n",
    "        Flatten(input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(len(indices_of_interest))\n",
    "    ])\n",
    "    xgb_model = XGBCustomObjective(\n",
    "        objective_func=sera_objective(piecewise_linear_phi_np(bounds, initial_weight=0.3)),\n",
    "        n_estimators=15,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    # assert len(regressors) == len(name_regressors)\n",
    "    # regressors =  [xgb_model, rnn16_model, lstm16_model, cnn_rnn_model, lp_model]\n",
    "    # name_regressors =  [\"CXGB15\", \"RNN16\", \"LSTM16\", \"CNNRNN16\", \"MLP16\"]\n",
    "    regressors= [xgb_model]\n",
    "    name_regressors= [\"CXGB15\"]\n",
    "    assert len(regressors) == len(name_regressors)\n",
    "    experiment_1 = PredictionExperiment(data, indices_of_interest, regressors, name_regressors, 5, id, loss_fn=SERA(bounds=bounds,T=100, initial_weight=0.3, fn=\"piecewise2\"))\n",
    "    experiment_1.execute_experiment()\n",
    "    experiment_1.get_metrics(\"r2\", \"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", \"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"TSCV\", show=False)\n",
    "\n",
    "    #experiment_1.top_results(\"r2\", 5, stage=\"prediction\", top_data_path=f\"data/results/{FREQUENCY}/top_results.csv\")\n",
    "    #experiment_1.top_results(\"cv_r2\", 5, stage=\"CV\", top_data_path=f\"data/results/{FREQUENCY}/top_results.csv\")\n",
    "    experiment_1.save_results(f\"data/sera_results_v2/{region}_results/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for id in id_experiments:\n",
    "    k+=1\n",
    "    print(\"Executing\",id, \"iter\", k)\n",
    "    data = {i: pd.read_parquet(f\"data/climate_features/{region}/predictor_{id}_{i}.parquet\") for i in range(1,13)}\n",
    "    rnn16_model = Sequential([\n",
    "    SimpleRNN(16, activation=\"tanh\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "    Dropout(0.1),  # Regularization\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(len(indices_of_interest))  # Predict 5 indices\n",
    "    ])\n",
    "    lstm16_model = Sequential([\n",
    "    LSTM(16, activation=\"tanh\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "    Dropout(0.1),  # Regularization\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(len(indices_of_interest))  # Predict 5 indices\n",
    "    ])\n",
    "    cnn_rnn_model = Sequential([\n",
    "        Conv1D(16, kernel_size=1, activation=\"relu\", input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "        Reshape((1, 16)),  # Back to time dimension\n",
    "        SimpleRNN(8, activation=\"tanh\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(len(indices_of_interest))\n",
    "    ])\n",
    "    lp_model = Sequential([\n",
    "        Flatten(input_shape=(1, len(data[1].columns) - len(indices_of_interest))),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(len(indices_of_interest))\n",
    "    ])\n",
    "    xgb_model = XGBCustomObjective(\n",
    "        objective_func=sera_objective(piecewise_linear_phi_np(bounds, initial_weight=0.3)),\n",
    "        n_estimators=15,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    # assert len(regressors) == len(name_regressors)\n",
    "    regressors =  [xgb_model, rnn16_model, lstm16_model, cnn_rnn_model, lp_model]\n",
    "    name_regressors =  [\"CXGB15\", \"RNN16\", \"LSTM16\", \"CNNRNN16\", \"MLP16\"]\n",
    "    assert len(regressors) == len(name_regressors)\n",
    "    experiment_1 = PredictionExperiment(data, indices_of_interest, regressors, name_regressors, 5, id, loss_fn=SERA(bounds=bounds,T=100, initial_weight=0.1, fn=\"piecewise2\"))\n",
    "    experiment_1.execute_experiment()\n",
    "    experiment_1.get_metrics(\"r2\", \"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", \"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"r2\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"mape\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"mae\", stage=\"TSCV\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"prediction\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"training\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"CV\", show=False)\n",
    "    experiment_1.get_metrics(\"sera\", stage=\"TSCV\", show=False)\n",
    "\n",
    "    #experiment_1.top_results(\"r2\", 5, stage=\"prediction\", top_data_path=f\"data/results/{FREQUENCY}/top_results.csv\")\n",
    "    #experiment_1.top_results(\"cv_r2\", 5, stage=\"CV\", top_data_path=f\"data/results/{FREQUENCY}/top_results.csv\")\n",
    "    experiment_1.save_results(f\"data/sera_results_v2/{region}_results/results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
