{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Notes:\n",
    "- \n",
    "\t- Seasonly prediction is with top20 features correlation per season, using total data except for last 5 months (per year) as training. then predicting 5 months (5 points in 5 years)\n",
    "\t- Yearly prediction is put together top20 features correlation per season and use the full 240 features in a single model for prediction. Using total data except last year (12 seasons) as training then predicting these last 12 months.\n",
    "\t- Respective Cali mean magnitudes: HWN: 0.5; HWF: 2.28; HWD: 1.81; HWM: 0.83; HWA: 1.46. (mean and std are kinda the same)\n",
    "\t- Respective Chile mean magnitudes: HWN: 0.29; HWF: 1.09; HWD: 0.89; HWM: 0.46; HWA: 0.85. (mean and std are kinda the same)\n",
    "\t- Note XGBoost is always overfitting.\n",
    "\n",
    "\n",
    "# First year comparison: (1990 vs 1995 vs 2000)\n",
    "## Chile:\n",
    "- Seasonly prediction:\n",
    "\t- More data implies more accuracy and less overfitting for most models, even when the features are better correlated\n",
    "\t- 1990 seems terrible for chile as first year, some months (2 and 9) have few predictors\n",
    "\t- see best models in 1990-1995-2000: using 1995 for Season 1 always get decent r^2, svc seems the best (0.66) one here overall 5 indices (XGboost overfits and 0.74). Season 2 and 3 looks bad for every starting year, except for season 2 using 2000 and RF (bad but positive R^2's).\n",
    "\n",
    "- Yearly prediction (seasonly features):\n",
    "\t- best performace is with a 1995 model (SVR-rbf) but usually better performance on HWN with 2000 data, 1995 better overall and 1990 is terrible.\n",
    "\t\n",
    "\n",
    "## California:\n",
    "- Seasonly prediction:\n",
    "\t- With more data seems more stable in R^2 measures, but best performance is from GPR for first biseason using 2000 as first year of data. RF seems pretty well for \tthird biseason. Second biseason looks terrible in both cases.\n",
    "\t- For winter, biseason 8 and 9 looks both terrible. For biseason 7 more data is better but quite bad (0.3-0.5 r^2). This is for all 3 starting years. \n",
    "\t- for 1990 data, season 1 is negative (1995 always positive). Biseason 2 looks good only for HWN. Season 3 good overall \n",
    "\t\n",
    "- Yearly prediction (seasonly features):\t\n",
    "\t- everything looks terrible, maybe code error? does not seem that bad for chile using same code. \n",
    "\t- same terrible for 1990 data, only xgboost seems a good option but overfits with more than 25 estimators.\n",
    "\n",
    "\n",
    "# Best year removing low variance\n",
    "## Chile:\n",
    "- Seasonly prediction:\n",
    "\t- Summer: Improved considerably for most indices in first biseason, even achieving 0.77 and 0.8 for HWM. Second and third biseasons are terrible anyways (were the same without removing).\n",
    "\t- Winter: Nothing relevant, some little improvements and reductions.\n",
    "\n",
    "- Yearly prediction (seasonly features):\n",
    "\t- Not a single r2 measure is better than without removing\n",
    "\n",
    "\n",
    "## California:\n",
    "- Seasonly prediction:\n",
    "- Yearly prediction (seasonly features):\n",
    "\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "- HWN is the easiest one to predict, HWD the hardest (some good results but overall terrible r^2). This is overall\n",
    "- Biseasons 1 and 7 are the easy to predict, always +0.4 r^2 measures. Biseasons 2 and 3 for chile are bad always, this could be biseason 2 lack of predictors for chile. California same but biseason 3 have some good results.\n",
    "- Summmer is easier to predict as Winter in both places.\n",
    "- 1995 seems overall the best starting year for predictors for chile. Surely evolution of HWs affects this conclusion: The prediction of next 5 years could be more correlated with the infofrom last 20 and not the full history. Is valid to create multiple model for multiple window years? Or should we build a single machine trained now for all predictions to 2100?\n",
    "- 1990 works the best for california yearly prediction\n",
    "- Removing by low variance works well for biseasonly prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reu 26/12:\n",
    "- Try linear regression using top1 predictor\n",
    "- include geopotential height 500\n",
    "- repeat study but using full data (1989 for cali, 1979 for chile using ttr, otherwise what era5)\n",
    "- use top10 features for the span of 50 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reu 02/01:\n",
    "- 10 stations from hugo were cs continuity of data per station, in 1988 one can see two stations lose data\n",
    "- check data stations in same platform as hugo to see and build more history for california"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATION INVENTORY FROM \n",
    "Western Regional Climate Center (WRCC)\n",
    "https://wrcc.dri.edu/Monitoring/Stations/station_inventory_show.php?snet=coop&sstate=CA \n",
    "\n",
    "- 040324    CA MONTEREY            ARROYO SECO GAGING STN         19530601 ACT  36 14 00 -121 29 00   801  \n",
    "- 040386    CA PLACER              AUBURN RS                      19530501 ACT  38 53 00 -121 04 00  1089  \n",
    "- 040675 04 CA SANTA CRUZ          BEN LOMOND WHITNEY             19310101 ACT  37 05 00 -122 06 00  1001  \n",
    "- 040852    CA SAN DIEGO           BLACK MTN CLEVELA              19530501 99991231  33 10 00 -116 48 00  4062  \n",
    "- 040952    CA ORANGE              BOLERO LO                      19560601 99991231  33 42 00 -117 39 00  1703  \n",
    "- 040998    CA SAN DIEGO           BOUCHER HILL                   19560701 99991231  33 20 00 -116 55 00  5453  \n",
    "\n",
    "\n",
    "Found NOAA also has, but limits 10year per station per request\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1972 Analysis Chile using top10 for 50 years of history\n",
    "\n",
    "- Biseasonly works terrible, except for biseason 7 on HWA using RF (0.47 R^2). Almost every other measure is close to zero or straight negative\n",
    "- Yearly R^2 are close to zero in every case\n",
    "\n",
    "If removing low variance (10%):\n",
    "- Biseasonly performance is terrible (couple cases negative but at most 0.23) \n",
    "- Yearly performance seems viable, most values are positive but highest one is just 0.57 using SVR, low variance filtering improves here, which is the contrary case to what we found using 1995 as start year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reu 07/01:\n",
    "\n",
    "- use vespinoza notebook for california stations, ask hugo how to build timeseries accordingly (detecting and filtering outliers) for the span needed: i'd say having 10 stations for the max possible history. Any station in california is good.\n",
    "- automatize prediction, that is, parametrize temporal resolution, number of predictors. Spatial resolution could be aswell. Shift data? \n",
    "- include other variables, like average sea temperature, forces/latitudes, MJO/MEI. \n",
    "- Check hypothesis until now: detrend data, boxes, pca, normalization, anomalies\n",
    "- cross validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From 1972 notes:\n",
    "\n",
    "## California\n",
    "- Cali year biseasonly features are always r^2 positive\n",
    "- Cali year yearly features all around but some close to 0.4 \n",
    "- Cali seasonly good for biseason 1, terrible for 2 and some 0.5 on 3. Same respectively for 7,8,9. Allthough 8 some cases 0.5 r^2.\n",
    "- Filtering low variance works well in Cali for biseasonly prediction, specially biseason 1 where 0.85 and 0.87 R^2 measures for HWN.\n",
    "\n",
    "## Chile\n",
    "- Chile is bad for the three cases from 1972\n",
    "- Chile gets positive when removing low variance only for year biseasonly features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From 1972 by Index notes:\n",
    "\n",
    "## California\n",
    "- year biseasonly feat looks the same\n",
    "- biseason feat improves very little on those already working well\n",
    "\n",
    "\n",
    "## Chile\n",
    "- works terrible either way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From 1972 top10 vs top20:\n",
    "\n",
    "## California\n",
    "- yearly features works better with top10, even linear achieves 0.63\n",
    "- seasonly pred works better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reu 14/01\n",
    "\n",
    "- check definition of HW, could be counting same HW multiple times\n",
    "- check distribution by winter or summer, to see if this matters in prediction\n",
    "- check predictions but using only first or first/second Modes\n",
    "- use box from land california, have to replace sst with t2m. same in chile and see\n",
    "\n",
    "- check why some biseason works well and other don't, could be label complexity \n",
    "- understand the physical phenomena and include other oscillations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
