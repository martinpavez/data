Open questions ML:
    Objective: Compare predicted HWs indices from ML prediction for different ssp scenarios. 
    1° Baseline:
        Detect in daily tasmax for each model, ensemble models for hw indices (monthly, yearly, decadal).
        Proposal is to work with 10 land points in the zone. Calculate hw indices for each point and average them (similar to averaging 10 stations).
        Models tasmax are on 100km resolution. chile average width is 180km and longitude zone is about 950km (rodelillo to PM). 
            Only have to be careful with geografic location of each point, i think the idea is the point simulate at most how the station in reality is.
    1° Model: Train in ERA5/Meteochile and predict first in historical (for validation with decadal stats), then in ssp:

        In historical predictions:
            Validate transferability from machine by predicting in historical experiments:
                - Ofc monthly would be terrible because of phasing, but get yearly/decadal statistics and compare
                - Compare feature importance on these predictions
                - 
        - Learned patterns from ERA5 could be applied to ssp. 
            This is a valid approach under the assumption that there is no big bias towards the relationship feature-labels in ERA5 across CMIP6 models (e.g. correlation era5 nino34-HWN vs cmip6 nino34-HWN)
            Maybe we would have to measure first how differ the relationship feature-labels (like in historical experiments). If they match, then the era5 machine would predict reasonably.
            How do we actually measure these relationships? We've been using pearson correlation but era5 best machine aint linear.
        Methodology:
            1. We have trained ERA5 machine
            2. Measure bias in CMIP6 models ?? Basically what we have on Taylor Diagrams? 
            3. Correct bias: Mean/variance scaling? Quantile Delta Mapping (QDM)? Equi-Distant Cumulative Density Function (EDCDF)? https://arxiv.org/pdf/2504.19145
            4. Build ssp features. Those using climatology periods, use the historical data from the same model.
            5. Predict and compare with 1° Baseline, get conclusions about ensemble indices across different scenarios.


    2° Model: Train in historical and predict ssp:
        - We just maintain the architecture of NNs (type, hyperparemeters, same monthly approach). So we train in historical (note it goes until 2014) and then predict in the same model.
            This would give a more comparable to the baseline (daily tasmax same model) 
        Methodology:
            1. Build ssp features from raw data.
            2. Labels should be an approximate from the same model to the 10stations from era5 machine' labels. 
                Can't use directly meteochile labels, because the historical models not necesarilly follow the exact timing, so the feature-label relationship does not match timewise.
                Label would be the same methodology as the 1° baseline.
            3. Train machine same hyperparameters as ERA5 but using these features-labels.
            4. Predict in scenarios and compare with 1° Baseline, get conclusions about ensemble indices across different scenarios.
        
        My biggest concern about this approach is the transferability of our ERA5 long-study to this approach. I think the transferability of hyperparameters is not strong, just a shortcut to find them.
        Disadvantage on using cmip6 labels, they are chaotic. So whats the point on training with these labels? Hard they beat the baseline.
        

Tasmax historical resolutions:
    100km : AWI, EC3, INM, MPI-M, MRI, GFDL
    250km : MIROC6


Anticyclone:
    leave it there, but be concious about daily drivers + aggregation (anyways f mention that cmip6 models are better on monthly scale)


Tasks:
    Create new experiment with most recent drivers:
        local_climate e.g.: calculate_kwargs="resample_freq=1MS;rolling_window=1;lon_box=(50,70);lat_box=(-10,10);anomalies=True"
        TOS:
            - local_climate/nino34 [190, 240, -5, 5] X
            - local_climate/nino12 [270, 280, -10, 0] X
            simplifications here use local_climate:
            - PDO is EOF on SSTanoms 120|240|20|60. Simplification: Anoms for now, then with jeremy pca X
                anom_pdo
            - DMI is the difference of SSTanoms (according to a base period not determined) from boxes 50|70|-10|10 and 90|110|-10|0 X
                So basically local_climate of tos for these boxes
                anom_dmi_east
                anom_dmi_west

        PSL:
            - pacific anticyclone (always_detect, 260,288,-45,-15) X calculate_kwargs="always_detect=True"
            - highlowdiff vaguada (b,A_cl,A_arg) X (default)
            - local_climate/wind_pressure_arg (both raco and puelche) [286, 292, -52, -40] X
            - SAM is the difference between standardize anomalies of psl40°S, psl65°S. That is:  X
                    Average longitude points across 40S and 65S (maybe local_climate box mean for a lat=40)
                    first remove the season climatology (1 mean value per season)
                    standardize the timeserie
                    remove the mean of baseline (1971-2001)
                    anom40 - anom65
                So for now:
                    anompsl40, anompsl65
                anom_psl_sam_40, anom_psl_sam_65

        UA or TA:
            - advection_mean/blob (anomalies, 268|283|-32|-20) 
            - advection_mean/chile (anomalies, 282|286|-42|-33), here change lat and lon box
            
            - local_climate/wind_cl_raco (raco) [287.5, 289.5, -37, -33]
            - local_climate/wind_cl_puelche (puelche) [286.5, 288.5, -42, -37]
            
            - local_climate/anom_ta_cl (anomalies, 286|288|-42|-33)   X

        

    3832cbd6:
        nino12
        nino34
        anticyclone
        anom_wind_cl
        anom_psl_A_Arg
        anom_psl_b
        anom_psl_A_cl
        3832cbd6_psl_1 (anom psl for winds in arg)


    create new experiment from ecf2577f but replace external indices with some simplification
    Test + validate these two experiments
    Train 3832cbd6, new 2 exps and predict in cmip6 data


sanity check (check hugo paper) for cmip6 historical
tasmax to intermediate



Issues:
    Handle cftime from inm model. should this be on raw to intermediate?

Questions:
    - How to compare labels? we are predicting monthly and compare decadal would mean some aggregation on these label predictions.
    For me, HWN and HWF is just sum over period. HWD and HWA would be the max of period.
    HWM would be the sum of HWF*HWM of month / sum of hwf
    Consider using sum of anomalies as index

    - Is there anything to measure on future scenarios on heatwave detection? any sanity check before predicting? Yes, check hw detection on pesimist scenarios indep by model and then ensemble mean
    For me its only the baseline to compare my predictions.
    
    - We used machines by monthy. Check if cmip6 represents the seasonality so we can build drivers and machines by season.


intermediate to primary for labels.
    Proposal: 
        tasmax on intermediate for gridded, parquets on observational by station.
        intermediate to primary would detect and detect + computation (so we can work on mauro's labels aswell)
        ref period is in question? remember fixed is decided, if not then add ref period as variable filter?
        Structure:
            03_primary/heatwaves
                /{index}/{method(meteochile or cxtp)}/variant
                /events/
            04_model_input/labels/{source}/regression_or_classification(detection or indices)/{index}/{method(meteochile or cxtp)}/variant

        variant would indicate spatial (one or several stations/points) range + ref period

        Software:
            Refactor processing (intermediate to primary) by source to accept driver or heatwaves parameter
            Labels dataset child Class



INM-CM5-0
MPI-ESM1-2-LR check resolution
IPSL-CM6A-LR check resolution
ACCESS-ESM1-5
CNRM-CM6-1 (ceda.uk)
CANESM5

all good monthly (historical + 126,245,370,585)


downloads:
monthly:
historical  psl, ta , ua XXXXXXXX
historical  tos  XXXXXXXXX
ssps psl XXXXXXXXXX
ssps ta XXXXXXXXXX
ssps ua XXXXXXXXX
ssps tos XXXXXXX

daily:
historical tasmax
126 tasmax
245 tasmax
370 tasmax
585 tasmax


drivers for ssp scenarios:
    concat historical + ssp and check the difference between those branches


New models:
    mpi match mpi
    inm match inm
    canesm5, access match ec earth


Tasks:
    Software for HW detection + stats so we can get baseline for CMIP6 models historical + scenarios 
    define new hw indices and establish mathematical relationships between them.

    Add detection for historical, to concatenate to ssps detection + predictions.

Final thoughts:
    Be sure that little changes on the machine (+- one driver, one label or initial parameters) does not affect the performance.


Tasks:
    Get new labels + new mathematical validation
    Confirm temporal resolution transform works on meteochile labels:
        Labels do not match monthly to yearly because of mean aggregation by stations, 
        so max(time) of mean(stations) of maximums(time) monthly != mean(stations) of maximums(time) yearly
    Repeat the ssp prediction plots + historical detection (not only historical prediction)

Test combinations of labels, maybe according to their mathematical relationship
    1° Iteration:
        - hwf,hwm,hwi: only improvement we see is in the mathematical validation (from around 0.4,0.6 in averaged time down to 0.1,0.2)
        - hwn, hwf, hwmeand: hwmeand looks better trends according to ssp scenarios but no extreme value prediction. Also improves mathematical validation, from fluctuating -0.2,0.2 down to -0.1,0.1.
        - hwn, hwi, hwmeani: hwn and hwi better trends (not really significant) and hwmeani looks flatter. mathematical validation looks the same (between -0.2,0).
        - hwd, hwa, hwmaxi: Nice improvement in shape, the machine risks more to predict extreme values, specially in hwd and hwa, while keeping the trends accross scenarios.
    
    2° Iteration (including era5 pred train, train 2022, dispersion) vs ssp_pred_2022 "full"
        - hwf, hwm, hwi: 
            1. pred vs det: hwf looks better on pred (worst scenarios have peaks that full does not), hwm is better adjusted to meteochile, so the pred is far better. hwi looks the same.
            2. dispersion: hwm has high dispersion (as in full), and one can see the positive trend on the mean models for this machine.
            3. validation: decent, between -0.2 and 0.1 for every ssp.
        - hwmeand, hwf, hwn:
            1. pred vs det: Adjust to meteochile looks the same, but we see better distinction between scenarios only for hwmeand.
            2. dispersion looks the same
            3. val looks good (-0.1,0.1)
        - hwn, hwi, hwmeani
            1. training looks better, so again the distinction between scenarios and trends are better.
            2. dispersion looks the same, but means on scenarios looks positive, while in full they look flat.
            3. val between -0.2 and 0.3
        - max labels: 
            1. training looks better only for hwa (closer to meteochile), so the trends on ssp pred looks good (kinda close to detection).
            2. both ssp trends are steeper on these machines, the dispersion range looks the same.
        
        - freq and intenstiy labels look the same as full, maybe 3 is kinda the magic number to adjust good our labels to meteochile (19 features).

(IDEA) Based on distribution?? so they match their SERA:
        - hwn, hwf, hwi
        - hwm, hwa, hwmeani, hwmaxi
        - hwd, hwmeand

Full Labels Problems:
    1. On dispersion plots, one can see that for each scenario, the mean models looks always flat (in comparison to their detection). 

Tasks:
    Build drivers for new cmip6 models (historical and ssps).


Tasks:
    Review ML (sera, hyperparameters, architecture)
    Incorporate era5 training prediction + detection in figure, so we can match how the model fits the meteochile curve.
    The idea is to improve historical cmip6 prediction (closer to meteochile than the historical detection).

    check how each model performs in the historical (vs meteochile), maybe 

    maybe transform labels into higher temporal resolution, so the phase effect is  neglected
        currently mean scenario is built on monthly predictions then to yearly labels !!!
            - Go to yearly first (or 5/10 yearly even) then mean of models!!

    Missing:
        1. Check plot results of label combinations (2° iteration)
        2. 5/10 years before models mean
        3. Maybe new intuition around SERA with the new distributions.
        
Tasks:
    1. include min,max range instead of mean or median + iqr:
        - Looks better than using dispersion, we can track how the range of the models reproduce meteochile.
        - I'd trust only the models where mainly ssp585 > ssp126
    2. choose representative model for the historical scenario. 
        - used mae for interpretation but can't average this to create a metric by model (as magnitudes of labels differ a lot)
        - used mape for averaging error to choose better model
        - anyways phase is still a problem as it goes yearly, we are comparing reality vs the model phase (point by point).
    3. develop a weighted average for the ssp prediction
        So if this is a good idea, we have to come up with a good metric to compare historical vs meteochile.

Tasks:
    1. Complete ERA5, meteochile data (1970-2024)
    2. Review SERA and architecture