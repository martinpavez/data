{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data of Maximum Temperature in California (NOAA)\n",
    "\n",
    "Author: Martin Pavez\n",
    "\n",
    "Creation: January 2025\n",
    "\n",
    "This notebook shows the very beginning steps in heatwave detection from meteorological stations data. \n",
    "1. Detection of missing data: quantification and cleaning.\n",
    "2. Selection of stations. \n",
    "3. We generate cleaned data for heatwave detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) # Suppress specific RuntimeWarnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Desktop\\data\\hw_extra\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "# change working directory to project's root path\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_PATH_AND_FILENAME = 'data/local_data/NOAA/stations.parquet'\n",
    "TEMP_DATA_PATH = 'data/local_data/NOAA/original/3899963.csv'\n",
    "CLEANED_DATA_PATH = 'data/local_data/NOAA/cleaned_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concatenate CSV files in a folder\n",
    "def concatenate_csv(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all .csv files in the specified folder, concatenates them (axis=0),\n",
    "    and indexes the resulting DataFrame by the 'date' column.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing .csv files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame indexed by the 'date' column.\n",
    "    \"\"\"\n",
    "    # List to store DataFrames from each CSV file\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Read CSV file, parse 'date' column as datetime\n",
    "            df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "            dataframes.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames along axis 0\n",
    "    concatenated_df = pd.concat(dataframes, axis=0)\n",
    "\n",
    "    # Set 'date' column as the index\n",
    "    concatenated_df.set_index('date', inplace=True)\n",
    "\n",
    "    ### Found duplicated items, these come from the end and start between files (some have them, some not)\n",
    "    #duplicates = concatenated_df.index[concatenated_df.index.duplicated(keep=False)]\n",
    "    #if not duplicates.empty:\n",
    "    #    print(\"Duplicate indices found:\")\n",
    "    #    print(duplicates)\n",
    "    concatenated_df = concatenated_df[~concatenated_df.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "    # Create a complete date range from the earliest to the latest date\n",
    "    complete_index = pd.date_range(start=concatenated_df.index.min(), end=concatenated_df.index.max(), freq='D')\n",
    "\n",
    "    # Reindex the DataFrame to the complete date range, filling gaps with NaN\n",
    "    concatenated_df = concatenated_df.reindex(complete_index)\n",
    "\n",
    "    # Rename the index to 'date'\n",
    "    concatenated_df.index.name = 'date'\n",
    "\n",
    "    # Sort by index for consistency\n",
    "    concatenated_df.sort_index(inplace=True)\n",
    "\n",
    "    concatenated_df[\"value\"] = concatenated_df[\"value\"]/10\n",
    "    concatenated_df.rename(columns={\"value\":\"max_temp\"}, inplace=True)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "def Month_Days(year):\n",
    "    return {\"year\": year, \"month_days\": {m: monthrange(year, m)[1] for m in range(1, 13)}}\n",
    "\n",
    "def Tfilter(data, column_label, nperc, year_window_init: int, year_window_end: int):\n",
    "    start_date = data.index[0]\n",
    "    end_date = data.index[-1]\n",
    "    perc_label = 'perc'\n",
    "    Tadd = 0.0\n",
    "    year_window_init = year_window_init\n",
    "    year_window_end = year_window_end\n",
    "    data_temp = data[column_label]\n",
    "\n",
    "    data_threshold = pd.DataFrame(\n",
    "        [],\n",
    "        columns=[perc_label],\n",
    "        index = data.index\n",
    "    )\n",
    "    for year in range(start_date.year, end_date.year + 1):\n",
    "        month_days = Month_Days(year)\n",
    "        for month in month_days[\"month_days\"]:\n",
    "            for day in range(1, month_days[\"month_days\"][month] + 1):\n",
    "                try:\n",
    "                    current_date = datetime(year, month, day)\n",
    "                except ValueError:\n",
    "                    if current_date == start_date:\n",
    "                        current_date = datetime(year, month, day+1)\n",
    "                    if current_date == end_date:\n",
    "                        current_date = datetime(year, month, day-1)\n",
    "                if  current_date >= start_date and current_date <= end_date:\n",
    "                    f_data_temp = data_temp[\n",
    "                        (year_window_init <= data_temp.index.year)\n",
    "                        & (data_temp.index.year <= year_window_end)\n",
    "                        & (data_temp.index.day == day)\n",
    "                        & (data_temp.index.month == month)\n",
    "                    ]\n",
    "                    try:\n",
    "                        data_threshold.loc[datetime(year, month, day), perc_label] = f_data_temp.quantile(\n",
    "                            nperc*0.01, interpolation=\"midpoint\"\n",
    "                        ).values[0]\n",
    "                    except AttributeError:\n",
    "                        data_threshold.loc[datetime(year, month, day), perc_label] = f_data_temp.quantile(\n",
    "                            nperc*0.01, interpolation=\"midpoint\"\n",
    "                        )\n",
    "                    except ValueError:\n",
    "                        data_threshold.loc[datetime(year, month, day-1), perc_label] = f_data_temp.quantile(\n",
    "                            nperc*0.01, interpolation=\"midpoint\"\n",
    "                        ).values[0] + Tadd\n",
    "\n",
    "        smoothed_series = data_threshold.rolling(window=31, center=True).mean()\n",
    "    return smoothed_series + Tadd\n",
    "    #return data_threshold + Tadd\n",
    "\n",
    "def to_format(data, max_temp_lim = 50, add_filter_year = None, filter_by_hist = False, filter = True, dropnans = True):\n",
    "\n",
    "    #data[\"Date\"] = pd.to_datetime(data[\"Date\"],format=\"%Y-%m-%d\")\n",
    "\n",
    "    if dropnans:\n",
    "        max_temp = data.set_index(\"date\").dropna(subset=[\"max_temp\"])[[\"max_temp\"]]\n",
    "\n",
    "    #max_temp = max_temp.rename(columns={\"DayAirTmpMax\":\"max_temp\"})\n",
    "    #min_temp = min_temp.rename(columns={\"DayAirTmpMin\":\"min_temp\"})\n",
    "    #mean_temp = mean_temp.rename(columns={\"DayAirTmpAvg\":\"mean_temp\"})\n",
    "    max_temp = data[[\"max_temp\"]]\n",
    "\n",
    "    if filter:\n",
    "\n",
    "        if add_filter_year is None:\n",
    "            max_temp = max_temp.drop(max_temp[np.abs(max_temp[\"max_temp\"])>max_temp_lim].index)\n",
    "        else:\n",
    "        #if add_filter_year is not None:\n",
    "            max_temp.loc[(max_temp.index.year < add_filter_year) & (np.abs(max_temp['max_temp']) > max_temp_lim), 'max_temp'] = np.nan\n",
    "            max_temp = max_temp.dropna(subset=[\"max_temp\"])[[\"max_temp\"]]\n",
    "\n",
    "\n",
    "        if filter_by_hist:#add_filter_year is not None:\n",
    "            perc_max = Tfilter(max_temp, 'max_temp', 99.9, add_filter_year, 2023)\n",
    "            perc_max = perc_max.reindex(max_temp.index)\n",
    "\n",
    "\n",
    "            max_temp.loc[(max_temp.index.year < add_filter_year) & (max_temp['max_temp'] > perc_max['perc'] + 10), 'max_temp'] = np.nan\n",
    "\n",
    "        if dropnans:\n",
    "            max_temp = max_temp.dropna(subset=[\"max_temp\"])[[\"max_temp\"]]\n",
    "    \n",
    "    else:\n",
    "        return data\n",
    "\n",
    "    concatenated_df = pd.concat([max_temp], axis=1)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "def separate_by_station(data, name_stat):\n",
    "    station_data = data[data[\"STATION\"]==name_stat]\n",
    "    station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
    "    station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
    "    station_data.set_index('Date', inplace=True)\n",
    "    # Create a complete date range from the earliest to the latest date\n",
    "    complete_index = pd.date_range(start=station_data.index.min(), end=station_data.index.max(), freq='D')\n",
    "\n",
    "    # Reindex the DataFrame to the complete date range, filling gaps with NaN\n",
    "    station_data = station_data.reindex(complete_index)\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMAX_ATTRIBUTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>19.4</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>19.4</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-03</td>\n",
       "      <td>17.8</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-04</td>\n",
       "      <td>16.7</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>17.2</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-06</td>\n",
       "      <td>16.1</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-07</td>\n",
       "      <td>18.3</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-08</td>\n",
       "      <td>19.4</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-09</td>\n",
       "      <td>17.2</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-10</td>\n",
       "      <td>12.2</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-11</td>\n",
       "      <td>16.1</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-12</td>\n",
       "      <td>13.3</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-13</td>\n",
       "      <td>18.9</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-14</td>\n",
       "      <td>16.1</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-15</td>\n",
       "      <td>20.0</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-16</td>\n",
       "      <td>16.1</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-17</td>\n",
       "      <td>16.7</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-18</td>\n",
       "      <td>16.7</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>USC00041253</td>\n",
       "      <td>CACHUMA LAKE, CA US</td>\n",
       "      <td>34.58223</td>\n",
       "      <td>-119.98159</td>\n",
       "      <td>240.5</td>\n",
       "      <td>1970-01-20</td>\n",
       "      <td>20.6</td>\n",
       "      <td>,,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        STATION                 NAME  LATITUDE  LONGITUDE  ELEVATION   \n",
       "0   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5  \\\n",
       "1   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "2   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "3   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "4   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "5   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "6   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "7   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "8   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "9   USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "10  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "11  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "12  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "13  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "14  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "15  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "16  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "17  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "18  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "19  USC00041253  CACHUMA LAKE, CA US  34.58223 -119.98159      240.5   \n",
       "\n",
       "          DATE  TMAX TMAX_ATTRIBUTES  \n",
       "0   1970-01-01  19.4             ,,0  \n",
       "1   1970-01-02  19.4             ,,0  \n",
       "2   1970-01-03  17.8             ,,0  \n",
       "3   1970-01-04  16.7             ,,0  \n",
       "4   1970-01-05  17.2             ,,0  \n",
       "5   1970-01-06  16.1             ,,0  \n",
       "6   1970-01-07  18.3             ,,0  \n",
       "7   1970-01-08  19.4             ,,0  \n",
       "8   1970-01-09  17.2             ,,0  \n",
       "9   1970-01-10  12.2             ,,0  \n",
       "10  1970-01-11  16.1             ,,0  \n",
       "11  1970-01-12  13.3             ,,0  \n",
       "12  1970-01-13  18.9             ,,0  \n",
       "13  1970-01-14  16.1             ,,0  \n",
       "14  1970-01-15  20.0             ,,0  \n",
       "15  1970-01-16  16.1             ,,0  \n",
       "16  1970-01-17  16.7             ,,0  \n",
       "17  1970-01-18  16.7             ,,0  \n",
       "18  1970-01-19  15.0             ,,0  \n",
       "19  1970-01-20  20.6             ,,0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all cimis stations information\n",
    "stations = pd.read_csv(TEMP_DATA_PATH)\n",
    "stations.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USC00041253',\n",
       " 'USC00041277',\n",
       " 'USC00041244',\n",
       " 'USC00040449',\n",
       " 'USC00040931',\n",
       " 'USC00040983',\n",
       " 'USC00040741',\n",
       " 'USC00040212',\n",
       " 'USC00041194',\n",
       " 'USC00040693']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter only stations that have data at least from 1971\n",
    "statlist = list(stations[\"STATION\"].unique())\n",
    "statlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USC00041253\n",
      "USC00041277\n",
      "USC00041244\n",
      "USC00040449\n",
      "USC00040931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USC00040983\n",
      "USC00040741\n",
      "USC00040212\n",
      "USC00041194\n",
      "USC00040693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data[\"DATE\"] = pd.to_datetime(station_data[\"DATE\"],format='%Y-%m-%d')\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_11104\\1310868446.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  station_data.rename(columns={\"DATE\":\"Date\", \"TMAX\":\"max_temp\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "stations_data_no_filter = {}\n",
    "stations_data_filter_nans = {}\n",
    "stations_data_filter1 = {}\n",
    "dropnans = False\n",
    "\n",
    "for stat in statlist:\n",
    "    print(stat)\n",
    "    station_data_to_read = separate_by_station(stations, stat)\n",
    "\n",
    "    stations_data_no_filter[stat] = to_format(station_data_to_read, max_temp_lim=50, add_filter_year=None, filter_by_hist = False, filter = False, dropnans=dropnans)\n",
    "    stations_data_filter1[stat] = to_format(station_data_to_read, max_temp_lim=50, add_filter_year=None, filter_by_hist = False, filter = True, dropnans=dropnans)\n",
    "    stations_data_filter_nans[stat] = to_format(station_data_to_read, max_temp_lim=50, add_filter_year=None, filter_by_hist = False, filter = False, dropnans=dropnans)\n",
    "\n",
    "    stations_data_filter1[stat].to_parquet(CLEANED_DATA_PATH + f'Stat_{stat}.parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total      nans  >50\n",
      "USC00041253  19693.0  0.015640  0.0\n",
      "USC00041277  19693.0  0.104098  0.0\n",
      "USC00041244  19693.0  0.062916  0.0\n",
      "USC00040449  19358.0  0.118976  0.0\n",
      "USC00040931  19693.0  0.049459  0.0\n",
      "USC00040983  19693.0  0.006246  0.0\n",
      "USC00040741  19693.0  0.036409  0.0\n",
      "USC00040212  19693.0  0.035393  0.0\n",
      "USC00041194  19693.0  0.013406  0.0\n",
      "USC00040693  19479.0  0.099579  0.0\n"
     ]
    }
   ],
   "source": [
    "#Percentage of missing and cleaned data\n",
    "\n",
    "df_nans_and_deleted = pd.DataFrame(index=statlist)\n",
    "df_total_days = {}\n",
    "df_filter1_days = {}\n",
    "total_days = np.zeros((len(statlist),))\n",
    "filter_nan_days = np.zeros((len(statlist),))\n",
    "filter1_days = np.zeros((len(statlist),))\n",
    "\n",
    "for i, stat in enumerate(statlist):\n",
    "    df_total_days = stations_data_no_filter[stat][stations_data_no_filter[stat].index.year>1970]['max_temp']\n",
    "    #np.isnan(station_data_to_read[stat]['temperature'])\n",
    "    df_filter_nan_days = stations_data_filter_nans[stat][stations_data_filter_nans[stat].index.year>1970]['max_temp']\n",
    "\n",
    "    df_filter1_days = stations_data_filter1[stat][stations_data_filter1[stat].index.year>1970]['max_temp']\n",
    "\n",
    "    #station_data_to_read[stat][np.isnan(station_data_to_read[stat]['temperature'])]\n",
    "    total_days[i] = len(df_total_days)#[np.isnan(df_total_days)])\n",
    "    filter_nan_days[i] = len(df_filter_nan_days[np.isnan(df_total_days)])\n",
    "    filter1_days[i] = len(df_filter1_days[np.isnan(df_filter1_days)])\n",
    "\n",
    "df_nans_and_deleted['total'] = np.array(total_days)\n",
    "df_nans_and_deleted['nans'] = np.array(filter_nan_days)/(np.array(total_days)[0])\n",
    "df_nans_and_deleted['>50'] = -np.array(filter_nan_days)/(np.array(total_days)[0])+np.array(filter1_days)/(np.array(total_days)[0])\n",
    "\n",
    "\n",
    "print(df_nans_and_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\\frac{total missing data}{total days} =$0.05405224099418335\n",
      "$\\frac{total missing data}{total days} =$0.0\n"
     ]
    }
   ],
   "source": [
    "print(r'$\\frac{total missing data}{total days} =$' +\n",
    "      str(np.sum(df_nans_and_deleted['nans']*df_nans_and_deleted['total'])/np.sum(df_nans_and_deleted['total'])))\n",
    "\n",
    "print(r'$\\frac{total missing data}{total days} =$' +\n",
    "      str(np.sum(df_nans_and_deleted['>50']*df_nans_and_deleted['total'])/np.sum(df_nans_and_deleted['total'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>nans</th>\n",
       "      <th>&gt;50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USC00040983</th>\n",
       "      <td>19693.0</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00041194</th>\n",
       "      <td>19693.0</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00041253</th>\n",
       "      <td>19693.0</td>\n",
       "      <td>0.015640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00040212</th>\n",
       "      <td>19693.0</td>\n",
       "      <td>0.035393</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00040741</th>\n",
       "      <td>19693.0</td>\n",
       "      <td>0.036409</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00040931</th>\n",
       "      <td>19693.0</td>\n",
       "      <td>0.049459</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00041244</th>\n",
       "      <td>19693.0</td>\n",
       "      <td>0.062916</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00040693</th>\n",
       "      <td>19479.0</td>\n",
       "      <td>0.099579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00041277</th>\n",
       "      <td>19693.0</td>\n",
       "      <td>0.104098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00040449</th>\n",
       "      <td>19358.0</td>\n",
       "      <td>0.118976</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               total      nans  >50\n",
       "USC00040983  19693.0  0.006246  0.0\n",
       "USC00041194  19693.0  0.013406  0.0\n",
       "USC00041253  19693.0  0.015640  0.0\n",
       "USC00040212  19693.0  0.035393  0.0\n",
       "USC00040741  19693.0  0.036409  0.0\n",
       "USC00040931  19693.0  0.049459  0.0\n",
       "USC00041244  19693.0  0.062916  0.0\n",
       "USC00040693  19479.0  0.099579  0.0\n",
       "USC00041277  19693.0  0.104098  0.0\n",
       "USC00040449  19358.0  0.118976  0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nans_and_deleted.sort_values(\"nans\", ascending=True, inplace=True)\n",
    "df_nans_and_deleted = df_nans_and_deleted[df_nans_and_deleted[\"total\"] > 18500]\n",
    "df_nans_and_deleted.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GHCND_USC00040983',\n",
       " 'GHCND_USC00041194',\n",
       " 'GHCND_USC00041253',\n",
       " 'GHCND_USC00040741',\n",
       " 'GHCND_USC00040212',\n",
       " 'GHCND_USC00040931',\n",
       " 'GHCND_USC00041277',\n",
       " 'GHCND_USC00041244',\n",
       " 'GHCND_USC00040693',\n",
       " 'GHCND_USC00040449']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statlist_10 = list(df_nans_and_deleted.iloc[0:10].index)\n",
    "statlist_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GHCND_USC00040983', 'GHCND_USC00041194', 'GHCND_USC00041253', 'GHCND_USC00040741', 'GHCND_USC00040212', 'GHCND_USC00040931', 'GHCND_USC00041277', 'GHCND_USC00041244', 'GHCND_USC00040693', 'GHCND_USC00040449']\n"
     ]
    }
   ],
   "source": [
    "print(str(statlist_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
